<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>参数初始化 | Xielei's Blog</title><meta name="author" content="xie lei"><meta name="copyright" content="xie lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="一、参数初始化1.1 原因参数初始化又称为权重初始化（weight initialization）或权值初始化。参数初始化指的是在网络模型训练之前，对各个节点的权重和偏置进行初始化赋值的过程，用于解决梯度消失或者梯度爆炸，有利于模型的收敛速度和性能表现, 同时也可以加入自己相关领域的先验进行权重初始化。当定义好网络模型之后，需要进行权重初始化，恰当的权重初始化方法，可以加快模型的收敛，不恰当的初始">
<meta property="og:type" content="article">
<meta property="og:title" content="参数初始化">
<meta property="og:url" content="http://example.com/2023/11/13/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96/index.html">
<meta property="og:site_name" content="Xielei&#39;s Blog">
<meta property="og:description" content="一、参数初始化1.1 原因参数初始化又称为权重初始化（weight initialization）或权值初始化。参数初始化指的是在网络模型训练之前，对各个节点的权重和偏置进行初始化赋值的过程，用于解决梯度消失或者梯度爆炸，有利于模型的收敛速度和性能表现, 同时也可以加入自己相关领域的先验进行权重初始化。当定义好网络模型之后，需要进行权重初始化，恰当的权重初始化方法，可以加快模型的收敛，不恰当的初始">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/logo.jpg">
<meta property="article:published_time" content="2023-11-13T04:34:30.000Z">
<meta property="article:modified_time" content="2023-11-13T15:28:57.885Z">
<meta property="article:author" content="xie lei">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="基础知识整理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/logo.jpg"><link rel="shortcut icon" href="/img/website_icon.png"><link rel="canonical" href="http://example.com/2023/11/13/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '参数初始化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-13 23:28:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Xielei's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/logo.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Xielei's Blog"><span class="site-name">Xielei's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">参数初始化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-13T04:34:30.000Z" title="发表于 2023-11-13 12:34:30">2023-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-13T15:28:57.885Z" title="更新于 2023-11-13 23:28:57">2023-11-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="参数初始化"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一、参数初始化"><a href="#一、参数初始化" class="headerlink" title="一、参数初始化"></a>一、参数初始化</h2><h3 id="1-1-原因"><a href="#1-1-原因" class="headerlink" title="1.1 原因"></a>1.1 原因</h3><p><strong>参数初始化</strong>又称为<strong>权重初始化</strong>（weight initialization）或<strong>权值初始化</strong>。参数初始化指的是在网络模型训练之前，对各个节点的权重和偏置进行初始化赋值的过程，用于解决梯度消失或者梯度爆炸，有利于模型的收敛速度和性能表现, 同时也可以加入自己相关领域的先验进行权重初始化。当定义好网络模型之后，需要进行权重初始化，恰当的权重初始化方法，可以加快模型的收敛，不恰当的初始化方法，可能导致梯度消失或爆炸，导致模型不可用。如果权重太小，则输入信号通过网络中的每一层时，其方差就会开始减小，输入最终会降低到非常低的值，导致梯度消失。如果权重太大，则输入数据的方差往往会随着每个传递层而迅速增加。最终，变得很大以至于梯度爆炸。</p>
<h3 id="1-2-参数初始化方法"><a href="#1-2-参数初始化方法" class="headerlink" title="1.2 参数初始化方法"></a>1.2 参数初始化方法</h3><p><strong>参数梯度不应该为0</strong>。而我们知道在全连接的神经网络中，参数梯度和反向传播得到的状态梯度以及入激活值有关——<strong>激活值饱和会导致该层状态梯度信息为0</strong>，然后导致下面所有层的参数梯度为0；<strong>入激活值为0会导致对应参数梯度为0</strong>。所以如果要保证<strong>参数梯度不等于0</strong>，那么参数初始化应该使得各层<strong>激活值不会出现饱和现象且激活值不为0</strong>。我们把这两个条件总结为参数初始化条件：</p>
<ul>
<li>初始化必要条件一：各层<strong>激活值不</strong>会出现<strong>饱和</strong>现象。</li>
<li>初始化必要条件二：各层激活值<strong>不为0</strong>。</li>
</ul>
<h4 id="（1）全0初始化"><a href="#（1）全0初始化" class="headerlink" title="（1）全0初始化"></a><strong>（1）全0初始化</strong></h4><p><strong>没有隐层时, 可以将所有的参数初始化为0，即深度模型都不会使用0初始化所有参数</strong></p>
<p>在神经网络中，把W初始化为0是不可以的。这是因为如果把W初始化，那么在前向传播过程中，每一层的神经元学到的东西都是一样的（激活值均为0），而在bp的时候，不同维度的参数会得到相同的更新，因为他们的gradient相同，称之为“对称失效”。同样常数初始化在这种情况也不行，因为gradient相同，weight update也相同，这样会令更新后的参数仍然保持一样的状态，但是可以初始化bias的值。</p>
<h4 id="（2）标准随机初始化"><a href="#（2）标准随机初始化" class="headerlink" title="（2）标准随机初始化"></a><strong>（2）标准随机初始化</strong></h4><p>希望所有参数的期望接近0。遵循这个原则，可以将参数设置为接近0的很小的随机数（有正有负），在实际中，随机参数服从高斯分布&#x2F;正态分布（Gaussian distribution &#x2F; normal distribution）和均匀分布（uniform distribution）都是有效的初始化方法。</p>
<p>但是一旦随机分布选择不当，就会导致网络优化陷入困境，引起梯度消失。</p>
<h4 id="（3）Glorot条件"><a href="#（3）Glorot条件" class="headerlink" title="（3）Glorot条件"></a><strong>（3）Glorot条件</strong></h4><ol>
<li>各个层的激活值h（输出值）的方差要保持一致</li>
<li>各个层对状态z的梯度的方差要保持一致</li>
</ol>
<h4 id="（4）参数初始化的几点要求"><a href="#（4）参数初始化的几点要求" class="headerlink" title="（4）参数初始化的几点要求"></a>（4）<strong>参数初始化的几点要求</strong></h4><ol>
<li><strong>参数不能全部初始化为0，也不能全部初始化同一个值</strong>；</li>
<li>最好保证参数初始化的<strong>均值为0，正负交错，正负参数大致上数量相等</strong>；</li>
<li>初始化参数<strong>不能太大或者是太小</strong>，参数太小会导致特征在每层间逐渐缩小而难以产生作用，参数太大会导致数据在逐层间传递时逐渐放大而导致梯度消失发散，不能训练；</li>
<li>如果有可能<strong>满足Glorot条件</strong>也是不错的；</li>
</ol>
<h3 id="1-3-seed与随机初始化"><a href="#1-3-seed与随机初始化" class="headerlink" title="1.3 seed与随机初始化"></a>1.3 seed与随机初始化</h3><p>seed在深度学习代码中叫随机种子，设置seed的目的是由于深度学习网络模型中初始的权值参数通常都是初始化成随机数。设置随机种子的方法能够近似的完全复现作者的开源深度学习代码，随机种子的选择能够减少一定程度上算法结果的随机性，即产生随机种子意味着每次运行实验，产生的随机数都是相同的吗，给复现算法提供了极大的帮助！</p>
<p>随机种子的设定对大部分的模型并不会产生特别大的实质性影响，神经网络更多会和迭代次数，学习率等相关。然而对于预训练模型而言，非常依赖参数的随机初始化过程，那么，随机种子的设定就显得非常重要。</p>
<p>随机种子输入分布生成函数tensor.normal_ , tensor.uniform_ 等，从而产生神经网络初始化参数。</p>
<h2 id="二、Xavier初始化"><a href="#二、Xavier初始化" class="headerlink" title="二、Xavier初始化"></a>二、Xavier初始化</h2><p>“Xavier”初始化方法是 2010 年提出的，针对有非线性激活函数时的权值初始化方法，是工程tricks中的一种，方法来源于论文<a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">《Understanding the difficulty of training deep feedforward neural networks》</a>，目的是为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。</p>
<ul>
<li>目标是保持数据的方差维持在 1 左右</li>
<li>针对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E9%A5%B1%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:3121171401%7D">饱和激活函数</a>如 sigmoid 和 tanh 等。</li>
</ul>
<p>整个大型前馈神经网络无非就是一个超级大映射，将原始样本<strong>稳定的</strong>映射成它的类别。也就是将样本空间映射到类别空间。试想，如果样本空间与类别空间的分布差异很大，比如说类别空间特别稠密，样本空间特别稀疏辽阔，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。同样，如果类别空间特别稀疏，样本空间特别稠密，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，<strong>也就是要让它们的方差尽可能相等</strong>。</p>
<p>优势：</p>
<ol>
<li><strong>梯度消失和爆炸</strong>：在深度网络中，梯度消失和梯度爆炸是一个常见的问题。如果每一层都将方差放大，那么在多层网络中，梯度可能会很快增长至非常大的值（爆炸），或者减小至接近零（消失）。Xavier 初始化试图使得每一层的输出的方差接近于其输入的方差，从而避免梯度消失或梯度爆炸的问题。</li>
<li><strong>加速收敛</strong>：Xavier 初始化使得每一层的输出的方差接近于其输入的方差，从而使得每一层的梯度的方差接近于 1。这样，每一层的参数更新的幅度就不会相差太大，从而加速收敛。</li>
</ol>
<p>劣势：</p>
<p>​			Xavier初始化主要用于tanh，softsign等奇函数、线性函数(Taylor展开)，不适用于ReLU，sigmod函数等非奇函数、线性函数。</p>
<p>原因在于Xavier方法的推导过程基于两点假设：</p>
<ul>
<li>(1) 激活函数是线性的，因此并不适应于ReLU，sigmoid等非线性激活函数</li>
<li>(2) 激活函数是关于0对称（奇函数）的，因此不适应于ReLU，sigmoid等不是关于0对称的激活函数</li>
</ul>
<h2 id="三、kaiming初始化"><a href="#三、kaiming初始化" class="headerlink" title="三、kaiming初始化"></a>三、kaiming初始化</h2><p>   Kaiming初始化的发明人kaiming he，在<a href="https://cloud.tencent.com/developer/tools/blog-entry?target=https://links.jianshu.com/go?to=https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>论文中提出了针对relu的kaiming初始化。 </p>
<p>Xavier在tanh函数上表现可以，但对 ReLU 等激活函数效果不好，何凯明引入了一种更鲁棒的权重初始化方法–He Initialization。</p>
<p>He Initialization也有两种变体：</p>
<p><strong>He Normal：</strong>正态分布的均值为0、方差为sqrt( 2&#x2F;fan_in )。</p>
<p><strong>He Uniform：</strong>均匀分布的区间为【-sqrt( 6&#x2F;fan_in) , sqrt(6&#x2F;fan_in) 】</p>
<p><strong>He Initialization适用于使用ReLU、Leaky ReLU这样的非线性激活函数的网络。</strong></p>
<p>He Initialization和Xavier Initialization 两种方法都使用类似的理论分析：它们为从中提取初始参数的分布找到了很好的方差。该方差适用于所使用的激活函数，并且在不明确考虑分布类型的情况下导出。</p>
<h2 id="四、pytorch参数初始化方法"><a href="#四、pytorch参数初始化方法" class="headerlink" title="四、pytorch参数初始化方法"></a>四、pytorch参数初始化方法</h2><p>pytorch中的各种参数层（Linear、Conv2d、BatchNorm等）在__init__方法中定义后，不需要手动初始化就可以直接使用，这是因为Pytorch对这些层都会进行默认初始化, 绝大部分为kaiming初始化yyds。</p>
<h3 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">kaiming_uniform_</span>(<span class="hljs-params">tensor, a=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;leaky_relu&#x27;</span></span>):<br>    fan = _calculate_correct_fan(tensor, mode)<br>    gain = calculate_gain(nonlinearity, a)<br>    std = gain / math.sqrt(fan)<br>    bound = math.sqrt(<span class="hljs-number">3.0</span>) * std  <span class="hljs-comment"># Calculate uniform bounds from standard deviation</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">return</span> tensor.uniform_(-bound, bound)<br></code></pre></td></tr></table></figure>

<p>kaiming_uniform_按照均匀分布初始化tensor，在U (−bound, bound)中采样，其中</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131240977.png" alt="img" style="zoom: 25%;" />

<p>同样的，fan_in在tensor为二维时，是tensor.size(1)，注意，上面给出的初始化公式均是在mode和nonlinearity在默认参数下的结果</p>
<h3 id="Linear的初始化"><a href="#Linear的初始化" class="headerlink" title="Linear的初始化"></a>Linear的初始化</h3><p>Linear自带的初始化函数为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>        init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="hljs-number">5</span>))<br>        <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)<br>            bound = <span class="hljs-number">1</span> / math.sqrt(fan_in)<br>            init.uniform_(self.bias, -bound, bound)<br></code></pre></td></tr></table></figure>

<p>W在U (−bound, bound)中采样，其中</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131240891.png" alt="img" style="zoom:25%;" />

<p>fan_in即为W的第二维大小，即Linear所作用的输入向量的维度</p>
<p>bias也在U (−bound,bound)中采样，且bound与W一样</p>
<h3 id="Conv的初始化"><a href="#Conv的初始化" class="headerlink" title="Conv的初始化"></a>Conv的初始化</h3><p>以二维为例，卷积层的参数实际上是一个四维tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> transposed:<br>    self.weight = Parameter(torch.Tensor(<br>        in_channels, out_channels // groups, *kernel_size))<br><span class="hljs-keyword">else</span>:<br>    self.weight = Parameter(torch.Tensor(<br>        out_channels, in_channels // groups, *kernel_size))<br><span class="hljs-keyword">if</span> bias:<br>    self.bias = Parameter(torch.Tensor(out_channels))<br><span class="hljs-keyword">else</span>:<br>    self.register_parameter(<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure>

<p>比如一个输入channel为3，输出channel为64，kernel size&#x3D;3的卷积层，其权值即为一个3×64×3×3的向量，它会这样进行初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>    init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="hljs-number">5</span>))<br>    <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)<br>        bound = <span class="hljs-number">1</span> / math.sqrt(fan_in)<br>        init.uniform_(self.bias, -bound, bound)<br></code></pre></td></tr></table></figure>

<p>同样默认使用kaiming_uniform，在U (−bound, bound)中采样，其中</p>
<img src="https://cdn.nlark.com/yuque/0/2023/png/33777925/1699794020071-ef3798da-f1e1-4848-b2eb-1e7ea823761d.png" alt="img" style="zoom:25%;" />

<p>对于fan_in的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">num_input_fmaps = tensor.size(<span class="hljs-number">1</span>)<br>num_output_fmaps = tensor.size(<span class="hljs-number">0</span>)<br>receptive_field_size = <span class="hljs-number">1</span><br><span class="hljs-keyword">if</span> tensor.dim() &gt; <span class="hljs-number">2</span>:<br>    receptive_field_size = tensor[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].numel()<br>fan_in = num_input_fmaps * receptive_field_size<br>fan_out = num_output_fmaps * receptive_field_size<br></code></pre></td></tr></table></figure>

<p>即：<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131242162.png" alt="img" style="zoom:25%;" /></p>
<h3 id="BatchNorm层初始化"><a href="#BatchNorm层初始化" class="headerlink" title="BatchNorm层初始化"></a>BatchNorm层初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>    self.reset_running_stats()<br>    <span class="hljs-keyword">if</span> self.affine:<br>        init.uniform_(self.weight)<br>        init.zeros_(self.bias)<br></code></pre></td></tr></table></figure>

<p>weight初始化为U (0, 1) ,bias初始化为0</p>
<h3 id="ResNet初始化"><a href="#ResNet初始化" class="headerlink" title="ResNet初始化"></a>ResNet初始化</h3><p>Resnet在定义各层之后，pytorch官方代码的__init__方法会对不同的层进行手动的初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>        nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):<br>        nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>        nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<p>首先对于所有卷积层，与之前不同，这里采用的mode是fan_out，nonlinearity是relu，且使用的初始化函数为kaiming_normal_，即参数在N (0,std)中采样，其中</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131243395.png" alt="img" style="zoom:25%;" />

<p>卷积层的bias这里没有提到，因此采用的仍然是默认的初始化方法，而BatchNorm和GroupNorm的weight均初始化为1，bias初始化为0，区别于默认的weight在0～1中均匀采样，bias为0，剩下的Linear层未被提到，仍然采用默认的初始化方法</p>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>VGG的pytorch官方初始化方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>            nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>            <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>            nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>            nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>            nn.init.normal_(m.weight, <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>            nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<p>卷积层的初始化方法同ResNet，只不过bias初始化为0，BatchNorm层初始化方法同ResNet，Linear层的weight初始化为N ( 0 , 0.01 ) N(0,0.01)N(0,0.01)，bias初始化为0</p>
<h2 id="五、pytorch初始化api"><a href="#五、pytorch初始化api" class="headerlink" title="五、pytorch初始化api"></a>五、pytorch初始化api</h2><h3 id="初始化方法汇总"><a href="#初始化方法汇总" class="headerlink" title="初始化方法汇总"></a>初始化方法汇总</h3><p>PyTorch 中提供了 11种初始化方法：</p>
<ol>
<li><ol>
<li>Xavier 均匀分布</li>
<li>Xavier 正态分布</li>
<li>Kaiming 均匀分布</li>
<li>Kaiming 正态分布</li>
<li>均匀分布</li>
<li>正态分布</li>
<li>常数分布</li>
<li>正交矩阵初始化</li>
<li>单位矩阵初始化</li>
<li>稀疏矩阵初始化</li>
<li>狄拉克δ函数初始化</li>
</ol>
</li>
</ol>
<h3 id="torch-nn-init-uniform-tensor-a-0-0-b-1-0"><a href="#torch-nn-init-uniform-tensor-a-0-0-b-1-0" class="headerlink" title="torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0)"></a>torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0)</h3><p>含义：从均匀分布 U ( a , b ) U(a, b)U(a,b)中生成值，填充输入的张量</p>
<p>参数：</p>
<p>（1）tensor - n 维的 torch.Tensor</p>
<p>（2）a - 均匀分布的下界</p>
<p>（3）b - 均匀分布的上界</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>torch.nn.init.uniform_(w, a=<span class="hljs-number">0</span>, b=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-normal-tensor-mean-0-std-1"><a href="#torch-nn-init-normal-tensor-mean-0-std-1" class="headerlink" title="torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)"></a>torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)</h3><p>含义：从给定均值和标准差的正态分布 N ( m e a n , s t d ) N(mean, std)N(mean,std)中生成值，填充输入的张量或变量</p>
<p>参数：</p>
<p>（1）tensor – n 维的 torch.Tensor</p>
<p>（2）mean – 正态分布的均值</p>
<p>（3）std – 正态分布的标准差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">from</span> torch <br><span class="hljs-keyword">import</span> nn w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>) <br>torch.nn.init.normal_(w, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-constant-tensor-val"><a href="#torch-nn-init-constant-tensor-val" class="headerlink" title="torch.nn.init.constant_(tensor, val)"></a>torch.nn.init.constant_(tensor, val)</h3><p>含义：用 val 的值填充输入的张量或变量</p>
<p>参数：</p>
<p>（1）tensor – n 维的 torch.Tensor</p>
<p>（2）val – 用来填充张量的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>torch.nn.init.constant_(w, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-eye-tensor"><a href="#torch-nn-init-eye-tensor" class="headerlink" title="torch.nn.init.eye_(tensor)"></a>torch.nn.init.eye_(tensor)</h3><p>含义：用单位矩阵来填充 2 维输入张量或变量。在线性层尽可能多的保存输入特性。<br>参数：<br>（1）tensor – 2 维的 torch.Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>torch.nn.init.eye_(w)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-dirac-tensor"><a href="#torch-nn-init-dirac-tensor" class="headerlink" title="torch.nn.init.dirac_(tensor)"></a>torch.nn.init.dirac_(tensor)</h3><p>含义：用 Dirac δ 函数来填充{3, 4, 5}维输入张量或变量。在卷积层尽可能多的保存输入通道特性。</p>
<p>参数：</p>
<p>（1）tensor – {3, 4, 5}维的 torch.Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>nn.init.dirac_(w)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-xavier-uniform-tensor-gain-1"><a href="#torch-nn-init-xavier-uniform-tensor-gain-1" class="headerlink" title="torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1)"></a>torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1)</h3><p>含义：用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采样自 U(-a, a),  该方法也被称为 Glorot initialisation。</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131244377.png" alt="img" style="zoom:25%;" />

<p>参数：</p>
<p>（1）tensor – n 维的 torch.Tensor</p>
<p>（2）gain - 可选的缩放因子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_uniform_(w)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-xavier-normal-tensor-gain-1-："><a href="#torch-nn-init-xavier-normal-tensor-gain-1-：" class="headerlink" title="torch.nn.init.xavier_normal_(tensor, gain&#x3D;1)："></a>torch.nn.init.xavier_normal_(tensor, gain&#x3D;1)：</h3><p>含义：用一个正态分布生成值，填充输入的张量或变量。结果张量中的值采样自:</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131245665.png" alt="img" style="zoom:25%;" />

<p>参数：</p>
<p>（1）tensor – n 维的 torch.Tensor</p>
<p>（2）gain - 可选的缩放因子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_normal_(w, gain=nn.init.calculate_gain(<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-kaiming-uniform-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’"><a href="#torch-nn-init-kaiming-uniform-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’" class="headerlink" title="torch.nn.init.kaiming_uniform_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)"></a>torch.nn.init.kaiming_uniform_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</h3><p>含义：用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采样自 U(-bound, bound)，其中</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131244176.png" alt="img" style="zoom:25%;" />

<p>参数：</p>
<p>（1）tensor：n 维的 torch.Tensor</p>
<p>（2）a：leaky_relu的负斜率，只有在nonlinearity&#x3D;’leaky_relu’时候起作用</p>
<p>（3）mode：fan_in或者fan_out。fan_in保留forward传递中权重的方差大小，fan_out保留backward传递中权重的方差大小</p>
<p>（4）nonlinearity：nn.functional名称，推荐使用relu和leaky_relu。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">from</span> torch <br><span class="hljs-keyword">import</span> nn w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>) <br>torch.nn.init.kaiming_uniform_(w, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’"><a href="#torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’" class="headerlink" title="torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)"></a>torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</h3><p>含义：用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采样自 U(-bound, bound)，其中</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131245548.png" alt="img" style="zoom:25%;" />

<p>参数：</p>
<p>（1）tensor：n 维的 torch.Tensor</p>
<p>（2）a：leaky_relu的负斜率，只有在nonlinearity&#x3D;’leaky_relu’时候起作用</p>
<p>（3）mode：fan_in或者fan_out。fan_in保留forward传递中权重的方差大小，fan_out保留backward传递中权重的方差大小</p>
<p>（4）nonlinearity：nn.functional名称，推荐使用relu和leaky_relu。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">from</span> torch <br><span class="hljs-keyword">import</span> nn w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>) <br>torch.nn.init.kaiming_uniform_(w, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-1"><a href="#torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-1" class="headerlink" title="torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)"></a>torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</h3><p>​	含义：用一个正态分布生成值，填充输入的张量或变量。结果张量中的值采样自:</p>
<img src="https://fastly.jsdelivr.net/gh/xieleixielei/picture/202311131248480.png" alt="img" style="zoom:25%;" />

<p>参数：</p>
<p>（1）tensor：n 维的 torch.Tensor</p>
<p>（2）a：leaky_relu的负斜率，只有在nonlinearity&#x3D;’leaky_relu’时候起作用</p>
<p>（3）mode：fan_in或者fan_out。fan_in保留forward传递中权重的方差大小，fan_out保留backward传递中权重的方差大小</p>
<p>（4）nonlinearity：nn.functional名称，推荐使用relu和leaky_relu。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.kaiming_normal_(w, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-sparse-tensor-sparsity-std-0-01"><a href="#torch-nn-init-sparse-tensor-sparsity-std-0-01" class="headerlink" title="torch.nn.init.sparse_(tensor, sparsity, std&#x3D;0.01)"></a>torch.nn.init.sparse_(tensor, sparsity, std&#x3D;0.01)</h3><p>含义：将 2 维的输入张量或变量当做稀疏矩阵填充，结果张量中的值采样自N(0,0.01)，其中非零元素根据一个均值为 0，标准差为 std 的正态分布生成。</p>
<p>参数：</p>
<p>（1）tensor – n 维的 torch.Tensor</p>
<p>（2）sparsity - 每列中需要被设置成零的元素比例</p>
<p>（3）std - 用于生成非零值的正态分布的标准差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.sparse(w, sparsity=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure>

<h3 id="torch-nn-init-orthogonal-tensor-gain-1"><a href="#torch-nn-init-orthogonal-tensor-gain-1" class="headerlink" title="torch.nn.init.orthogonal_(tensor, gain&#x3D;1)"></a>torch.nn.init.orthogonal_(tensor, gain&#x3D;1)</h3><p>含义：用一个(半)正交矩阵初始化输入张量，参考Saxe, A. et al. (2013) - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6120">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a>。输入张量必须至少有2维，对于大于2维的张量，超出的维度将被flatten化。正交初始化可以使得卷积核更加紧凑，可以去除相关性，使模型更容易学到有效的参数。</p>
<p>参数：</p>
<p>（1）tensor – n维需要初始化的张量，其中n&gt;&#x3D;2<br>（2）gain -可选放缩因子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">from</span> torch <br><span class="hljs-keyword">import</span> nn  w = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>) <br>torch.nn.init.orthogonal_(w)<br></code></pre></td></tr></table></figure>

<h3 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h3><p>可以用apply()函数初始化，可选用pytorch提供的多种初始化函数，apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">weights_init</span>(<span class="hljs-params">m</span>):<br>    classname=m.__class__.__name__<br>    <span class="hljs-keyword">if</span> classname.find(<span class="hljs-string">&#x27;Conv&#x27;</span>) != -<span class="hljs-number">1</span>:<br>        xavier(m.weight.data)<br>        xavier(m.bias.data)<br>net = Net()<br>net.apply(weights_init) <span class="hljs-comment">#apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。   </span><br></code></pre></td></tr></table></figure>







<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/shuzfan/article/details/51338178">深度学习——Xavier初始化方法_xavier_uniform_-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27919794">深度前馈网络与Xavier初始化原理</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64464584">深度学习：Xavier and Kaiming Initialization</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39653948/article/details/107950764#Paper_3">【精选】PyTorch中的Xavier以及He权重初始化方法解释_pytorch中he初始化-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/336005430?utm_id=0">深度学习之参数初始化</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/luo3300612/article/details/97675312">Pytorch 默认参数初始化_pytorch中默认的参数初始化-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43229348/article/details/120332308">PyTorch常用函数(8)_nn.init.calculate_gain-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42766639/article/details/125061803">seed在模型中的应用及用法_模型的seed什么作用_zyrlia的博客-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/545344518">【调参侠的修炼笔记2】随机种子Seed的讲人话解释</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43679439/article/details/125134408">模型初始化与随机种子——Pytorch 炼丹技巧（随手记）_pytorch需要随机初始化吗-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/355337178">权重&#x2F;参数初始化方法</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/416922998">如何选择合适的初始化方法 | 神经网络的初始化方法总结</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">xie lei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/11/13/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96/">http://example.com/2023/11/13/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Xielei's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/">基础知识整理</a></div><div class="post_share"><div class="social-share" data-image="/img/logo.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/09/python%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" title="python内存管理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">python内存管理</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/02/mAP%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/" title="mAP及相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">mAP及相关概念</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/01/%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="主动学习笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-01</div><div class="title">主动学习笔记</div></div></a></div><div><a href="/2023/11/01/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/" title="常用数据集汇总"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-01</div><div class="title">常用数据集汇总</div></div></a></div><div><a href="/2023/11/02/mAP%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/" title="mAP及相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-02</div><div class="title">mAP及相关概念</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">xie lei</div><div class="author-info__description">朝闻道兮</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xieleixielei"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">一、参数初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 参数初始化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%85%A80%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">（1）全0初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%A0%87%E5%87%86%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">（2）标准随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Glorot%E6%9D%A1%E4%BB%B6"><span class="toc-number">1.2.3.</span> <span class="toc-text">（3）Glorot条件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%87%A0%E7%82%B9%E8%A6%81%E6%B1%82"><span class="toc-number">1.2.4.</span> <span class="toc-text">（4）参数初始化的几点要求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-seed%E4%B8%8E%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 seed与随机初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">二、Xavier初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81kaiming%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.</span> <span class="toc-text">三、kaiming初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81pytorch%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">四、pytorch参数初始化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">初始化函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.2.</span> <span class="toc-text">Linear的初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conv%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.3.</span> <span class="toc-text">Conv的初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BatchNorm%E5%B1%82%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.4.</span> <span class="toc-text">BatchNorm层初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text">ResNet初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VGG"><span class="toc-number">4.6.</span> <span class="toc-text">VGG</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81pytorch%E5%88%9D%E5%A7%8B%E5%8C%96api"><span class="toc-number">5.</span> <span class="toc-text">五、pytorch初始化api</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB"><span class="toc-number">5.1.</span> <span class="toc-text">初始化方法汇总</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-uniform-tensor-a-0-0-b-1-0"><span class="toc-number">5.2.</span> <span class="toc-text">torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-normal-tensor-mean-0-std-1"><span class="toc-number">5.3.</span> <span class="toc-text">torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-constant-tensor-val"><span class="toc-number">5.4.</span> <span class="toc-text">torch.nn.init.constant_(tensor, val)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-eye-tensor"><span class="toc-number">5.5.</span> <span class="toc-text">torch.nn.init.eye_(tensor)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-dirac-tensor"><span class="toc-number">5.6.</span> <span class="toc-text">torch.nn.init.dirac_(tensor)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-xavier-uniform-tensor-gain-1"><span class="toc-number">5.7.</span> <span class="toc-text">torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-xavier-normal-tensor-gain-1-%EF%BC%9A"><span class="toc-number">5.8.</span> <span class="toc-text">torch.nn.init.xavier_normal_(tensor, gain&#x3D;1)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-kaiming-uniform-tensor-a-0-mode-%E2%80%99fan-in%E2%80%99-nonlinearity-%E2%80%99leaky-relu%E2%80%99"><span class="toc-number">5.9.</span> <span class="toc-text">torch.nn.init.kaiming_uniform_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-kaiming-normal-tensor-a-0-mode-%E2%80%99fan-in%E2%80%99-nonlinearity-%E2%80%99leaky-relu%E2%80%99"><span class="toc-number">5.10.</span> <span class="toc-text">torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-kaiming-normal-tensor-a-0-mode-%E2%80%99fan-in%E2%80%99-nonlinearity-%E2%80%99leaky-relu%E2%80%99-1"><span class="toc-number">5.11.</span> <span class="toc-text">torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-sparse-tensor-sparsity-std-0-01"><span class="toc-number">5.12.</span> <span class="toc-text">torch.nn.init.sparse_(tensor, sparsity, std&#x3D;0.01)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-nn-init-orthogonal-tensor-gain-1"><span class="toc-number">5.13.</span> <span class="toc-text">torch.nn.init.orthogonal_(tensor, gain&#x3D;1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.14.</span> <span class="toc-text">自定义初始化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%EF%BC%9A"><span class="toc-number"></span> <span class="toc-text">参考：</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/70-%E7%88%AC%E6%A5%BC%E6%A2%AF/" title="70. 爬楼梯">70. 爬楼梯</a><time datetime="2024-07-16T12:07:31.000Z" title="发表于 2024-07-16 20:07:31">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/19-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E5%80%92%E6%95%B0%E7%AC%AC-N-%E4%B8%AA%E7%BB%93%E7%82%B9/" title="19. 删除链表的倒数第 N 个结点">19. 删除链表的倒数第 N 个结点</a><time datetime="2024-07-16T08:00:48.000Z" title="发表于 2024-07-16 16:00:48">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/2956-%E6%89%BE%E5%88%B0%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E5%85%83%E7%B4%A0/" title="2956.找到两个数组中的公共元素">2956.找到两个数组中的公共元素</a><time datetime="2024-07-16T07:28:48.000Z" title="发表于 2024-07-16 15:28:48">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/" title="206.反转链表">206.反转链表</a><time datetime="2024-07-16T06:56:29.000Z" title="发表于 2024-07-16 14:56:29">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/" title="2.两数相加">2.两数相加</a><time datetime="2024-07-16T02:56:20.000Z" title="发表于 2024-07-16 10:56:20">2024-07-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By xie lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>