<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP论文复现-paddle指南 | Xielei's Blog</title><meta name="author" content="xie lei"><meta name="copyright" content="xie lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="论文复现赛指南-NLP方向本文为针对 NLP 方向的复现赛指南 1. 总览1.1 背景 以深度学习为核心的人工智能技术仍在高速发展，通过论文复现，开发者可以获得    学习成长：自我能力提升 技术积累：对科研或工作有所帮助和启发 社区荣誉：成果被开发者广泛使用    1.2 前序工作基于本指南复现论文过程中，建议开发者准备以下内容。  了解该模型输入输出格式。以BERT的情感分类任务为例，通过阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP论文复现-paddle指南">
<meta property="og:url" content="http://example.com/2024/01/10/NLP%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-paddle%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="Xielei&#39;s Blog">
<meta property="og:description" content="论文复现赛指南-NLP方向本文为针对 NLP 方向的复现赛指南 1. 总览1.1 背景 以深度学习为核心的人工智能技术仍在高速发展，通过论文复现，开发者可以获得    学习成长：自我能力提升 技术积累：对科研或工作有所帮助和启发 社区荣誉：成果被开发者广泛使用    1.2 前序工作基于本指南复现论文过程中，建议开发者准备以下内容。  了解该模型输入输出格式。以BERT的情感分类任务为例，通过阅读">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/logo.jpg">
<meta property="article:published_time" content="2024-01-10T02:15:16.000Z">
<meta property="article:modified_time" content="2024-01-10T02:16:40.536Z">
<meta property="article:author" content="xie lei">
<meta property="article:tag" content="paddle">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/logo.jpg"><link rel="shortcut icon" href="/img/website_icon.png"><link rel="canonical" href="http://example.com/2024/01/10/NLP%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-paddle%E6%8C%87%E5%8D%97/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP论文复现-paddle指南',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-10 10:16:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Xielei's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/logo.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Xielei's Blog"><span class="site-name">Xielei's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP论文复现-paddle指南</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-10T02:15:16.000Z" title="发表于 2024-01-10 10:15:16">2024-01-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-10T02:16:40.536Z" title="更新于 2024-01-10 10:16:40">2024-01-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>38分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP论文复现-paddle指南"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="论文复现赛指南-NLP方向"><a href="#论文复现赛指南-NLP方向" class="headerlink" title="论文复现赛指南-NLP方向"></a>论文复现赛指南-NLP方向</h1><p>本文为针对 <code>NLP</code> 方向的复现赛指南</p>
<h2 id="1-总览"><a href="#1-总览" class="headerlink" title="1. 总览"></a>1. 总览</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><ul>
<li><p>以深度学习为核心的人工智能技术仍在高速发展，通过论文复现，开发者可以获得 </p>
</li>
<li><ul>
<li>学习成长：自我能力提升</li>
<li>技术积累：对科研或工作有所帮助和启发</li>
<li>社区荣誉：成果被开发者广泛使用</li>
</ul>
</li>
</ul>
<h3 id="1-2-前序工作"><a href="#1-2-前序工作" class="headerlink" title="1.2 前序工作"></a>1.2 前序工作</h3><p>基于本指南复现论文过程中，建议开发者准备以下内容。</p>
<ul>
<li><p>了解该模型输入输出格式。以BERT的情感分类任务为例，通过阅读论文与参考代码，了解到模型输入为<code>[batch_size, sequence_length]</code>的tensor，类型为<code>int64</code>，label为<code>[batch, ]</code>的label，类型为<code>int64</code>。</p>
</li>
<li><p>准备好训练&#x2F;验证数据集，用于模型训练与评估</p>
</li>
<li><p>准备好fake input data以及label，与模型输入shape、type等保持一致，用于后续模型前向对齐。 </p>
</li>
<li><ul>
<li>在对齐模型前向过程中，我们不需要考虑数据集模块等其他模块，此时使用fake data是将模型结构和数据部分解耦非常合适的一种方式。</li>
<li>将fake data以文件的形式存储下来，也可以保证PaddlePaddle与参考代码的模型结构输入是完全一致的，更便于排查问题。</li>
<li>在该步骤中，以BERT为例，生成fake data的脚本可以参考：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/fake_data/gen_fake_data.py">gen_fake_data.py</a>。</li>
</ul>
</li>
<li><p>在特定设备(CPU&#x2F;GPU)上，跑通参考代码的预测过程(前向)以及至少2轮(iteration)迭代过程，保证后续基于PaddlePaddle复现论文过程中可对比。</p>
</li>
<li><p>本文档基于 <code>BERT-SST2-Prod</code> 代码以及<code>reprod_log</code> whl包进行说明与测试。如果希望体验，建议参考<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/README.md">BERT-SST2-Prod文档</a>进行安装与测试。</p>
</li>
<li><p>在复现的过程中，只需要将PaddlePaddle的复现代码以及打卡日志上传至github，不能在其中添加<code>参考代码的实现</code>，在验收通过之后，需要删除打卡日志。建议在初期复现的时候，就将<strong>复现代码与参考代码分成2个文件夹进行管理</strong>。</p>
</li>
<li><p>飞桨训推一体认证 (Training and Inference Pipeline Certification, TIPC) 是一个针对飞桨模型的测试工具，方便用户查阅每种模型的训练推理部署打通情况，并可以进行一键测试。论文训练对齐之后，需要为代码接入TIPC基础链条测试文档与代码，关于TIPC基础链条测试接入规范的文档可以参考：<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/models/blob/tipc/docs/tipc_test/development_specification_docs/train_infer_python.md">链接</a>。更多内容在<code>3.13</code>章节部分也会详细说明。</p>
</li>
</ul>
<h2 id="2-整体框图"><a href="#2-整体框图" class="headerlink" title="2. 整体框图"></a>2. 整体框图</h2><h3 id="2-1-流程概览"><a href="#2-1-流程概览" class="headerlink" title="2.1 流程概览"></a>2.1 流程概览</h3><p>面对一篇自然语言处理的论文，复现该论文的整体流程如下图所示。</p>
<p><img src="https://gitee.com/xieleileileilei/picture/raw/master/image/1704852475151-da141b98-07da-437c-93fc-ca4c416e33ec.png" alt="img"></p>
<p>总共包含12个步骤。为了高效复现论文，设置了6个验收节点。如上图中黄色框所示。后续章节会详细介绍上述步骤和验收节点，具体内容安排如下：</p>
<ul>
<li>第3章：介绍12个复现步骤的理论知识、实战以及验收流程。</li>
<li>第4章：针对复现流程过程中每个步骤可能出现的问题，本章会进行详细介绍。</li>
</ul>
<h3 id="2-2-reprod-log-whl包"><a href="#2-2-reprod-log-whl包" class="headerlink" title="2.2 reprod_log whl包"></a>2.2 reprod_log whl包</h3><h4 id="2-2-1-reprod-log工具简介"><a href="#2-2-1-reprod-log工具简介" class="headerlink" title="2.2.1 reprod_log工具简介"></a>2.2.1 reprod_log工具简介</h4><p><code>reprod_log</code>是用于论文复现赛中辅助自查和验收工具。该工具源代码地址在：<a target="_blank" rel="noopener" href="https://github.com/WenmuZhou/reprod_log%E3%80%82%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E5%A6%82%E4%B8%8B%EF%BC%9A">https://github.com/WenmuZhou/reprod_log。主要功能如下：</a></p>
<ul>
<li>存取指定节点的输入输出tensor</li>
<li>基于文件的tensor读写</li>
<li>2个字典的对比验证</li>
<li>对比结果的输出与记录</li>
</ul>
<p>更多API与使用方法可以参考：<a target="_blank" rel="noopener" href="https://github.com/WenmuZhou/reprod_log/blob/master/README.md">reprod_log API使用说明</a>。</p>
<h4 id="2-2-2-reprod-log使用demo"><a href="#2-2-2-reprod-log使用demo" class="headerlink" title="2.2.2 reprod_log使用demo"></a>2.2.2 reprod_log使用demo</h4><p>下面基于代码：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline/reprod_log_demo%EF%BC%8C%E7%BB%99%E5%87%BA%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AF%A5%E5%B7%A5%E5%85%B7%E3%80%82">https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline/reprod_log_demo，给出如何使用该工具。</a></p>
<p>文件夹中包含<code>write_log.py</code>和<code>check_log_diff.py</code>文件，其中<code>write_log.py</code>中给出了<code>ReprodLogger</code>类的使用方法，<code>check_log_diff.py</code>给出了<code>ReprodDiffHelper</code>类的使用方法，依次运行两个python文件，使用下面的方式运行代码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">进入文件夹</span><br>cd pipeline/reprod_log_demo<br><span class="hljs-meta prompt_"># </span><span class="language-bash">随机生成矩阵，写入文件中</span><br>python write_log.py<br><span class="hljs-meta prompt_"># </span><span class="language-bash">进行文件对比，输出日志</span><br>python check_log_diff.py<br></code></pre></td></tr></table></figure>

<p>最终会输出以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">[2021/11/18 09:29:31] root INFO: demo_test_1:<br>[2021/11/18 09:29:31] root INFO:     mean diff: check passed: True, value: 0.0<br>[2021/11/18 09:29:31] root INFO: demo_test_2:<br>[2021/11/18 09:29:31] root INFO:     mean diff: check passed: False, value: 0.33387675881385803<br>[2021/11/18 09:29:31] root INFO: diff check failed<br></code></pre></td></tr></table></figure>



<p>可以看出：对于key为<code>demo_test_1</code>的矩阵，由于diff为0，小于设置的阈值<code>1e-6</code>，核验成功；对于key为<code>demo_test_2</code>的矩阵，由于diff为0.33，大于设置的阈值<code>1e-6</code>，核验失败。</p>
<h4 id="2-2-3-reprod-log在论文复现中应用"><a href="#2-2-3-reprod-log在论文复现中应用" class="headerlink" title="2.2.3 reprod_log在论文复现中应用"></a>2.2.3 reprod_log在论文复现中应用</h4><p>在论文复现中，基于reprod_log的结果记录模块，产出下面若干文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plain">log_reprod<br>├── forward_paddle.npy<br>├── forward_torch.npy    # 与forward_paddle.npy作为一并核查的文件对<br>├── metric_paddle.npy<br>├── metric_torch.npy     # 与metric_paddle.npy作为一并核查的文件对<br>├── loss_paddle.npy<br>├── loss_torch.npy       # 与loss_paddle.npy作为一并核查的文件对<br>├── bp_align_paddle.npy<br>├── bp_align_torch.npy   # 与bp_align_paddle.npy作为一并核查的文件对<br>├── train_align_paddle.npy<br>├── train_align_torch.npy # pytorch运行得到的参考评估指标<br></code></pre></td></tr></table></figure>

<p>基于reprod_log的<code>ReprodDiffHelper</code>模块，产出下面5个日志文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">├── forward_diff.log     # forward_paddle.npy与forward_torch.npy生成的diff结果文件<br>├── metric_diff.log      # metric_paddle.npy与metric_torch.npy生成的diff结果文件<br>├── loss_diff.log          # loss_paddle.npy与loss_torch.npy生成的diff结果文件<br>├── bp_align_diff.log    # bp_align_paddle.npy与bp_align_torch.npy生成的diff结果文件<br>├── train_align_diff.log # train_align_paddle.train_align_torch.npy生成的diff结果文件<br></code></pre></td></tr></table></figure>

<p>上述文件的生成代码都需要开发者进行开发，验收时需要提供上面罗列的所有文件（不需要提供产生这些文件的可运行程序）以及完整的模型训练评估程序和日志。</p>
<p>BERT-SST2-Prod项目提供了基于reprod_log的5个验收点对齐验收示例，具体代码地址为：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%AD%E7%9A%84README.md%E6%96%87%E6%A1%A3%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%E3%80%82">https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline，每个文件夹中的README.md文档提供了使用说明。</a></p>
<p>InsightFace项目中提供了<code>TIPC基础链条验收点</code>的验收示例，参考代码地址为：<a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_paddle/test_tipc/readme.md%EF%BC%8C%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8ETIPC%E5%9F%BA%E7%A1%80%E9%93%BE%E6%9D%A1%E6%B5%8B%E8%AF%95%E6%8E%A5%E5%85%A5%E8%A7%84%E8%8C%83%E7%9A%84%E4%BB%A3%E7%A0%81%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83%EF%BC%9Ahttps://github.com/PaddlePaddle/models/blob/tipc/docs/tipc_test/development_specification_docs/train_infer_python.md">https://github.com/deepinsight/insightface/blob/master/recognition/arcface_paddle/test_tipc/readme.md，更多关于TIPC基础链条测试接入规范的代码可以参考：https://github.com/PaddlePaddle/models/blob/tipc/docs/tipc_test/development_specification_docs/train_infer_python.md</a></p>
<h2 id="3-论文复现理论知识及实战"><a href="#3-论文复现理论知识及实战" class="headerlink" title="3. 论文复现理论知识及实战"></a>3. 论文复现理论知识及实战</h2><h3 id="3-1-模型结构对齐"><a href="#3-1-模型结构对齐" class="headerlink" title="3.1 模型结构对齐"></a>3.1 模型结构对齐</h3><p>对齐模型结构时，一般有3个主要步骤：</p>
<ul>
<li>网络结构代码转换</li>
<li>权重转换</li>
<li>模型组网正确性验证</li>
</ul>
<p>下面详细介绍这3个部分。</p>
<h4 id="3-1-1-网络结构代码转换"><a href="#3-1-1-网络结构代码转换" class="headerlink" title="3.1.1 网络结构代码转换"></a>3.1.1 网络结构代码转换</h4><p><strong>【基本流程】</strong></p>
<p>由于PyTorch的API和PaddlePaddle的API非常相似，可以参考<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_api_mapping/pytorch_api_mapping_cn.html">PyTorch-PaddlePaddle API映射表</a><br>，组网部分代码直接进行手动转换即可。</p>
<p><strong>【实战】</strong></p>
<p>BERT网络结构的PyTorch实现: <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py">transformers-bert</a></p>
<p>对应转换后的PaddlePaddle实现: <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/bert/modeling.py">paddlenlp-bert</a></p>
<h4 id="3-1-2-权重转换"><a href="#3-1-2-权重转换" class="headerlink" title="3.1.2 权重转换"></a>3.1.2 权重转换</h4><p><strong>【基本流程】</strong></p>
<p>组网代码转换完成之后，需要对模型权重进行转换，如果PyTorch repo中已经提供权重，那么可以直接下载并进行后续的转换；如果没有提供，则可以基于PyTorch代码，随机生成一个初始化权重(定义完model以后，使用<code>torch.save()</code> API保存模型权重)，然后进行权重转换。</p>
<p><strong>【注意事项】</strong></p>
<p>在权重转换的时候，需要注意<code>paddle.nn.Linear</code>以及<code>paddle.nn.BatchNorm2D</code>等API的权重保存格式和名称等与PyTorch稍有diff，具体内容可以参考<code>4.1章节</code>。</p>
<p><strong>【实战】</strong></p>
<p>BERT的代码转换脚本可以在这里查看：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/weights/torch2paddle.py%EF%BC%8C">https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/weights/torch2paddle.py，</a></p>
<p>注意：运行该代码需要首先下载Huggingface的BERT预训练模型到该目录下，下载地址为：<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-uncased/blob/main/pytorch_model.bin">https://huggingface.co/bert-base-uncased/blob/main/pytorch_model.bin</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/weights/torch2paddle.py</span><br><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> paddlenlp.transformers <span class="hljs-keyword">import</span> BertForPretraining <span class="hljs-keyword">as</span> PDBertForMaskedLM<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertForMaskedLM <span class="hljs-keyword">as</span> PTBertForMaskedLM<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_pytorch_checkpoint_to_paddle</span>(<span class="hljs-params"></span><br><span class="hljs-params">        pytorch_checkpoint_path=<span class="hljs-string">&quot;pytorch_model.bin&quot;</span>,</span><br><span class="hljs-params">        paddle_dump_path=<span class="hljs-string">&quot;model_state.pdparams&quot;</span>,</span><br><span class="hljs-params">        version=<span class="hljs-string">&quot;old&quot;</span>, </span>):<br>    hf_to_paddle = &#123;<br>        <span class="hljs-string">&quot;embeddings.LayerNorm&quot;</span>: <span class="hljs-string">&quot;embeddings.layer_norm&quot;</span>,<br>        <span class="hljs-string">&quot;encoder.layer&quot;</span>: <span class="hljs-string">&quot;encoder.layers&quot;</span>,<br>        <span class="hljs-string">&quot;attention.self.query&quot;</span>: <span class="hljs-string">&quot;self_attn.q_proj&quot;</span>,<br>        <span class="hljs-string">&quot;attention.self.key&quot;</span>: <span class="hljs-string">&quot;self_attn.k_proj&quot;</span>,<br>        <span class="hljs-string">&quot;attention.self.value&quot;</span>: <span class="hljs-string">&quot;self_attn.v_proj&quot;</span>,<br>        <span class="hljs-string">&quot;attention.output.dense&quot;</span>: <span class="hljs-string">&quot;self_attn.out_proj&quot;</span>,<br>        <span class="hljs-string">&quot;intermediate.dense&quot;</span>: <span class="hljs-string">&quot;linear1&quot;</span>,<br>        <span class="hljs-string">&quot;output.dense&quot;</span>: <span class="hljs-string">&quot;linear2&quot;</span>,<br>        <span class="hljs-string">&quot;attention.output.LayerNorm&quot;</span>: <span class="hljs-string">&quot;norm1&quot;</span>,<br>        <span class="hljs-string">&quot;output.LayerNorm&quot;</span>: <span class="hljs-string">&quot;norm2&quot;</span>,<br>        <span class="hljs-string">&quot;predictions.decoder.&quot;</span>: <span class="hljs-string">&quot;predictions.decoder_&quot;</span>,<br>        <span class="hljs-string">&quot;predictions.transform.dense&quot;</span>: <span class="hljs-string">&quot;predictions.transform&quot;</span>,<br>        <span class="hljs-string">&quot;predictions.transform.LayerNorm&quot;</span>: <span class="hljs-string">&quot;predictions.layer_norm&quot;</span>,<br>    &#125;<br>    do_not_transpose = []<br>    <span class="hljs-keyword">if</span> version == <span class="hljs-string">&quot;old&quot;</span>:<br>        hf_to_paddle.update(&#123;<br>            <span class="hljs-string">&quot;predictions.bias&quot;</span>: <span class="hljs-string">&quot;predictions.decoder_bias&quot;</span>,<br>            <span class="hljs-string">&quot;.gamma&quot;</span>: <span class="hljs-string">&quot;.weight&quot;</span>,<br>            <span class="hljs-string">&quot;.beta&quot;</span>: <span class="hljs-string">&quot;.bias&quot;</span>,<br>        &#125;)<br>        do_not_transpose = do_not_transpose + [<span class="hljs-string">&quot;predictions.decoder.weight&quot;</span>]<br><br>    pytorch_state_dict = torch.load(<br>        pytorch_checkpoint_path, map_location=<span class="hljs-string">&quot;cpu&quot;</span>)<br>    paddle_state_dict = OrderedDict()<br>    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> pytorch_state_dict.items():<br>        is_transpose = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> k[-<span class="hljs-number">7</span>:] == <span class="hljs-string">&quot;.weight&quot;</span>:<br>            <span class="hljs-comment"># embeddings.weight and LayerNorm.weight do not transpose</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>(d <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> do_not_transpose):<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;.embeddings.&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;.LayerNorm.&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k:<br>                    <span class="hljs-keyword">if</span> v.ndim == <span class="hljs-number">2</span>:<br>                        v = v.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>                        is_transpose = <span class="hljs-literal">True</span><br>        oldk = k<br>        <span class="hljs-keyword">for</span> hf_name, pd_name <span class="hljs-keyword">in</span> hf_to_paddle.items():<br>            k = k.replace(hf_name, pd_name)<br><br>        <span class="hljs-comment"># add prefix `bert.`</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;bert.&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;cls.&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;classifier&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> k:<br>            k = <span class="hljs-string">&quot;bert.&quot;</span> + k<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Converting: <span class="hljs-subst">&#123;oldk&#125;</span> =&gt; <span class="hljs-subst">&#123;k&#125;</span> | is_transpose <span class="hljs-subst">&#123;is_transpose&#125;</span>&quot;</span>)<br>        paddle_state_dict[k] = v.data.numpy()<br><br>    paddle.save(paddle_state_dict, paddle_dump_path)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compare</span>(<span class="hljs-params">out_torch, out_paddle</span>):<br>    out_torch = out_torch.detach().numpy()<br>    out_paddle = out_paddle.detach().numpy()<br>    <span class="hljs-keyword">assert</span> out_torch.shape == out_paddle.shape<br>    abs_dif = np.<span class="hljs-built_in">abs</span>(out_torch - out_paddle)<br>    mean_dif = np.mean(abs_dif)<br>    max_dif = np.<span class="hljs-built_in">max</span>(abs_dif)<br>    min_dif = np.<span class="hljs-built_in">min</span>(abs_dif)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mean_dif:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(mean_dif))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;max_dif:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(max_dif))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;min_dif:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(min_dif))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_forward</span>():<br>    paddle.set_device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    model_torch = PTBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;./bert-base-uncased&quot;</span>)<br>    model_paddle = PDBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;./bert-base-uncased&quot;</span>)<br>    model_torch.<span class="hljs-built_in">eval</span>()<br>    model_paddle.<span class="hljs-built_in">eval</span>()<br>    np.random.seed(<span class="hljs-number">42</span>)<br>    x = np.random.randint(<br>        <span class="hljs-number">1</span>, model_paddle.bert.config[<span class="hljs-string">&quot;vocab_size&quot;</span>], size=(<span class="hljs-number">4</span>, <span class="hljs-number">64</span>))<br>    input_torch = torch.tensor(x, dtype=torch.int64)<br>    out_torch = model_torch(input_torch)[<span class="hljs-number">0</span>]<br><br>    input_paddle = paddle.to_tensor(x, dtype=paddle.int64)<br>    out_paddle = model_paddle(input_paddle)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;torch result shape:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(out_torch.shape))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle result shape:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(out_paddle.shape))<br>    compare(out_torch, out_paddle)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    convert_pytorch_checkpoint_to_paddle(<br>        <span class="hljs-string">&quot;./bert-base-uncased/pytorch_model.bin&quot;</span>,<br>        <span class="hljs-string">&quot;./bert-base-uncased/model_state.pdparams&quot;</span>)<br>    test_forward()<br>    <span class="hljs-comment"># torch result shape:torch.Size([4, 64, 30522])</span><br>    <span class="hljs-comment"># paddle result shape:[4, 64, 30522]</span><br>    <span class="hljs-comment"># mean_dif:1.666686512180604e-05</span><br>    <span class="hljs-comment"># max_dif:0.00015211105346679688</span><br>    <span class="hljs-comment"># min_dif:0.0</span><br></code></pre></td></tr></table></figure>



<p>运行完成之后，会在当前目录生成<code>model_state.pdparams</code>文件，即为转换后的PaddlePaddle预训练模型。<br><strong>Tips</strong>: 由于paddlenlp中已有转换后的bert-base-uncased模型，因此可以一键加载，程序会自动下载对应权重！</p>
<h4 id="3-1-3-模型组网正确性验证"><a href="#3-1-3-模型组网正确性验证" class="headerlink" title="3.1.3 模型组网正确性验证"></a>3.1.3 模型组网正确性验证</h4><p><strong>【基本流程】</strong></p>
<ol>
<li>定义PyTorch模型，加载权重，固定seed，基于numpy生成随机数，转换为PyTorch可以处理的tensor，送入网络，获取输出，使用reprod_log保存结果。</li>
<li>定义PaddlePaddle模型，加载权重，固定seed，基于numpy生成随机数，转换为PaddlePaddle可以处理的tensor，送入网络，获取输出，使用reprod_log保存结果。</li>
<li>使用reprod_log排查diff，小于阈值，即可完成自测。</li>
</ol>
<p><strong>【注意事项】</strong></p>
<ul>
<li>模型在前向对齐验证时，需要调用<code>model.eval()</code>方法，保证组网中的随机量被关闭，比如BatchNorm、Dropout等。</li>
<li>给定相同的输入数据，为保证可复现性，如果有随机数生成，固定相关的随机种子。</li>
<li>输出diff可以使用<code>np.mean(np.abs(o1 - o2))</code>进行计算，一般小于1e-6的话，可以认为前向没有问题。如果最终输出结果diff较大，可以使用二分的方法进行排查，比如说BERT，包含1个embdding层、12个transformer-block以及最后的MLM head层，那么完成模型组网和权重转换之后，如果模型输出没有对齐，可以尝试输出中间某一个transformer-block的tensor进行对比，如果相同，则向后进行排查；如果不同，则继续向前进行排查，以此类推，直到找到导致没有对齐的操作。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>BERT模型组网正确性验证可以参考如下示例代码：<br><a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline/Step1">https://github.com/JunnYu/BERT-SST2-Prod/tree/main/pipeline/Step1</a></p>
<p><strong>【验收】</strong></p>
<p>对于待复现的项目，前向对齐验收流程如下。</p>
<ol>
<li>准备输入：fake data</li>
</ol>
<ul>
<li><ul>
<li>使用参考代码的dataloader，生成一个batch的数据，保存下来，在前向对齐时，直接从文件中读入。</li>
<li>固定随机数种子，生成numpy随机矩阵，转化tensor</li>
</ul>
</li>
</ul>
<ol>
<li>保存输出：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle&#x2F;PyTorch：dict，key为tensor的name（自定义），value为tensor的值。最后将dict保存到文件中。建议命名为<code>forward_paddle.npy</code>和<code>forward_torch.npy</code>。</li>
</ul>
</li>
</ul>
<ol>
<li>自测：使用reprod_log加载2个文件，使用report功能，记录结果到日志文件中，建议命名为<code>forward_diff_log.txt</code>，观察diff，二者diff小于特定的阈值即可。</li>
<li>提交内容：新建文件夹，将<code>forward_paddle.npy</code>、<code>forward_torch.npy</code>与<code>forward_diff_log.txt</code>文件放在文件夹中，后续的输出结果和自查日志也放在该文件夹中，一并打包上传即可。</li>
<li>注意：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle与PyTorch保存的dict的key需要保持相同，否则report过程可能会提示key无法对应，从而导致report失败，之后的<code>【验收】</code>环节也是如此。</li>
<li>如果是固定随机数种子，建议将fake data保存到dict中，方便check参考代码和PaddlePaddle的输入是否一致。</li>
</ul>
</li>
</ul>
<h3 id="3-2-验证-测试集数据读取对齐"><a href="#3-2-验证-测试集数据读取对齐" class="headerlink" title="3.2 验证&#x2F;测试集数据读取对齐"></a>3.2 验证&#x2F;测试集数据读取对齐</h3><p><strong>【基本流程】</strong></p>
<p>对于一个数据集，一般有以下一些信息需要重点关注</p>
<ul>
<li>数据集名称、下载地址</li>
<li>训练集&#x2F;验证集&#x2F;测试集</li>
</ul>
<p>PaddlePaddle中数据集相关的API为<code>paddle.io.Dataset</code>，PyTorch中对应为<code>torch.utils.data.Dataset</code>，二者功能一致，在绝大多数情况下，可以使用该类构建数据集。它是描述Dataset方法和行为的抽象类，在具体实现的时候，需要继承这个基类，实现其中的<code>__getitem__</code>和<code>__len__</code>方法。除了参考代码中相关实现，也可以参考待复现论文中的说明。</p>
<p>复现完Dataset之后，可以构建Dataloader，对数据进行组batch、批处理，送进网络进行计算。</p>
<p><code>paddle.io.DataLoader</code>可以进行数据加载，将数据分成批数据，并提供加载过程中的采样。PyTorch对应的实现为<code>torch.utils.data.DataLoader</code>，二者在功能上一致，只是在参数方面稍有diff：（1）PaddlePaddle缺少对<code>pin_memory</code>等参数的支持；（2）PaddlePaddle增加了<code>use_shared_memory</code>参数来选择是否使用共享内存加速数据加载过程。</p>
<p><strong>【注意事项】</strong></p>
<p>论文中一般会提供数据集的名称以及基本信息。复现过程中，我们在下载完数据之后，建议先检查下是否和论文中描述一致，否则可能存在的问题有：</p>
<ul>
<li>数据集版本不同，比如论文中使用了cnn_dailymail的v3.0.0版本数据集，但是我们下载的是cnn_dailymail的v1.0.0版本数据集，如果不对其进行检查，可能会导致我们最终训练的数据量等与论文中有diff</li>
<li>数据集使用方式不同，有些论文中，可能只是抽取了该数据集的子集进行方法验证，此时需要注意抽取方法，需要保证抽取出的子集完全相同。</li>
<li>在评估指标对齐时，我们可以固定batch size，关闭Dataloader的shuffle操作。</li>
</ul>
<p>构建数据集时，可以使用PaddleNLP中的数据集加载方式，具体可以参考：<a target="_blank" rel="noopener" href="https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html">如何自定义数据集</a>。对应地，PyTorch中的数据处理api可以参考：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/about_dataset_load.html#building-a-dataset">huggingface的datasets自定义数据集</a>。对于其中之一，可以找到另一个平台的实现。</p>
<p>此外，</p>
<ul>
<li>有些自定义的数据处理方法，如果不涉及到深度学习框架的部分，可以直接复用。</li>
<li>对于特定任务中的数据预处理方法，比如说Tokenizer，如果没有现成的API可以调用，可以参考PaddleNLP套件中的一些实现方法，比如<code>BertTokenizer</code>, <code>XLNetTokenizer</code>等。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>BERT模型复现过程中，数据预处理和Dataset、Dataloader的检查可以参考该文件：<br><a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step2/test_data.py">https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step2/test_data.py</a></p>
<p>使用方法可以参考<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step2/README.md">数据检查文档</a>。</p>
<h3 id="3-3-评估指标对齐"><a href="#3-3-评估指标对齐" class="headerlink" title="3.3 评估指标对齐"></a>3.3 评估指标对齐</h3><p><strong>【基本流程】</strong></p>
<p>PaddlePaddle提供了一系列Metric计算类，比如说<code>Accuracy</code>, <code>Auc</code>, <code>Precision</code>, <code>Recall</code>等，而PyTorch中，目前可以通过组合的方式实现metric计算，或者调用<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/about_metrics.html?highlight=metric">huggingface-datasets</a>，在论文复现的过程中，需要注意保证对于该模块，给定相同的输入，二者输出完全一致。具体流程如下。</p>
<ol>
<li>构建fake数据</li>
<li>使用PyTorch的指标获取评估结果，使用reprod_log保存结果。</li>
<li>使用PaddlePaddle的指标获取评估结果，使用reprod_log保存结果。</li>
<li>使用reprod_log排查diff，小于阈值，即可完成自测。</li>
</ol>
<p><strong>【注意事项】</strong></p>
<p>在评估指标对齐之前，需要注意保证对于该模块，给定相同的输入，二者输出完全一致。</p>
<p><strong>【实战】</strong></p>
<p>评估指标对齐检查方法可以参考文档：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step2/README.md#%E6%95%B0%E6%8D%AE%E8%AF%84%E4%BC%B0%E5%AF%B9%E9%BD%90%E6%B5%81%E7%A8%8B">评估指标对齐检查方法文档</a></p>
<p><strong>【验收】</strong></p>
<p>对于待复现的项目，评估指标对齐验收流程如下。</p>
<ol>
<li>输入：dataloader, model</li>
<li>输出：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle&#x2F;PyTorch：dict，key为tensor的name（自定义），value为具体评估指标的值。最后将dict使用reprod_log保存到各自的文件中，建议命名为<code>metric_paddle.npy</code>和<code>metric_torch.npy</code>。</li>
<li>自测：使用reprod_log加载2个文件，使用report功能，记录结果到日志文件中，建议命名为<code>metric_diff_log.txt</code>，观察diff，二者diff小于特定的阈值即可。</li>
</ul>
</li>
</ul>
<ol>
<li>提交内容：将<code>metric_paddle.npy</code>、<code>metric_torch.npy</code>与<code>metric_diff_log.txt</code>文件备份到<code>3.1节验收环节</code>新建的文件夹中，后续的输出结果和自查日志也放在该文件夹中，一并打包上传即可。</li>
<li>注意：</li>
</ol>
<ul>
<li><ul>
<li>数据需要是真实数据</li>
<li>需要检查论文是否只是抽取了验证集&#x2F;测试集中的部分文件，如果是的话，则需要保证PaddlePaddle和参考代码中dataset使用的数据集一致。</li>
</ul>
</li>
</ul>
<h3 id="3-4-损失函数对齐"><a href="#3-4-损失函数对齐" class="headerlink" title="3.4 损失函数对齐"></a>3.4 损失函数对齐</h3><p><strong>【基本流程】</strong></p>
<p>PaddlePaddle与PyTorch均提供了很多loss function，用于模型训练，具体的API映射表可以参考：<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_api_mapping/pytorch_api_mapping_cn.html#lossapi">Loss类API映射列表</a>。以CrossEntropyLoss为例，主要区别为：</p>
<ul>
<li>PaddlePaddle提供了对软标签、指定softmax计算纬度的支持。</li>
</ul>
<p>如果论文中使用的loss function没有指定的API，则可以尝试通过组合API的方式，实现自定义的loss function。</p>
<p>具体流程如下。</p>
<ol>
<li>定义PyTorch模型，加载权重，加载fake data 和 fake label（或者固定seed，基于numpy生成随机数），转换为PyTorch可以处理的tensor，送入网络，获取loss结果，使用reprod_log保存结果。</li>
<li>定义PaddlePaddle模型，加载fake data 和 fake label（或者固定seed，基于numpy生成随机数），转换为PaddlePaddle可以处理的tensor，送入网络，获取loss结果，使用reprod_log保存结果。</li>
<li>使用reprod_log排查diff，小于阈值，即可完成自测。</li>
</ol>
<p><strong>【注意事项】</strong></p>
<ul>
<li>计算loss的时候，建议设置<code>model.eval()</code>，避免模型中随机量的问题。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>本部分可以参考文档：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step3/README.md%E3%80%82">https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step3/README.md。</a></p>
<p><strong>【验收】</strong></p>
<p>对于待复现的项目，损失函数对齐验收流程如下。</p>
<ol>
<li>输入：fake data &amp; label</li>
<li>输出：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle&#x2F;PyTorch：dict，key为tensor的name（自定义），value为具体评估指标的值。最后将dict使用reprod_log保存到各自的文件中，建议命名为<code>loss_paddle.npy</code>和<code>loss_torch.npy</code>。</li>
</ul>
</li>
</ul>
<ol>
<li>自测：使用reprod_log加载2个文件，使用report功能，记录结果到日志文件中，建议命名为<code>loss_diff_log.txt</code>，观察diff，二者diff小于特定的阈值即可。</li>
<li>提交内容：将<code>loss_paddle.npy</code>、<code>loss_torch.npy</code>与<code>loss_diff_log.txt</code>文件备份到<code>3.1节验收环节</code>新建的文件夹中，后续的输出结果和自查日志也放在该文件夹中，一并打包上传即可。</li>
</ol>
<h3 id="3-5-优化器对齐"><a href="#3-5-优化器对齐" class="headerlink" title="3.5 优化器对齐"></a>3.5 优化器对齐</h3><p><strong>【基本流程】</strong></p>
<p>PaddlePaddle中的optimizer有<code>paddle.optimizer</code>等一系列实现，PyTorch中则有<code>torch.Optim</code>等一系列实现。</p>
<p><strong>【注意事项】</strong></p>
<p>以SGD等优化器为例，PaddlePaddle与Pytorch的优化器区别主要如下。</p>
<ul>
<li>PaddlePaddle在优化器中增加了对梯度裁剪的支持，在训练GAN或者一些NLP、多模态任务中，这个用到的比较多。</li>
<li>PaddlePaddle的SGD不支持动量更新、动量衰减和Nesterov动量，这里需要使用<code>paddle.optimizer.Momentum</code> API实现这些功能。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>本部分对齐建议对照<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Overview_cn.html">PaddlePaddle优化器API文档</a>与参考代码的优化器实现进行对齐，用之后的反向对齐统一验证该模块的正确性。</p>
<h3 id="3-6-学习率对齐"><a href="#3-6-学习率对齐" class="headerlink" title="3.6 学习率对齐"></a>3.6 学习率对齐</h3><p><strong>【基本流程】</strong></p>
<ul>
<li>学习率策略主要用于指定训练过程中的学习率变化曲线，这里可以将定义好的学习率策略，不断step，即可得到对应的学习率值，可以将学习率值保存在列表或者矩阵中，使用<code>reprod_log</code>工具判断二者是否对齐。</li>
</ul>
<p><strong>【注意事项】</strong></p>
<p>PaddlePaddle中，需要首先构建学习率策略，再传入优化器对象中；对于PyTorch，如果希望使用更丰富的学习率策略，需要先构建优化器，再传入学习率策略类API。</p>
<p><strong>【实战】</strong></p>
<p>学习率复现对齐，可以参考代码：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step4/README.md#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E9%BD%90%E9%AA%8C%E8%AF%81">学习率对齐验证文档</a>。</p>
<h3 id="3-7-正则化策略对齐"><a href="#3-7-正则化策略对齐" class="headerlink" title="3.7 正则化策略对齐"></a>3.7 正则化策略对齐</h3><p><strong>【基本流程】</strong></p>
<p>L2正则化策略用于模型训练，可以防止模型对训练数据过拟合，L1正则化可以用于得到稀疏化的权重矩阵，PaddlePaddle中有<code>paddle.regularizer.L1Decay</code>与<code>paddle.regularizer.L2Decay</code> API。PyTorch中，torch.optim集成的优化器只有L2正则化方法，直接在构建optimizer的时候，传入<code>weight_decay</code>参数即可。</p>
<p><strong>【注意事项】</strong></p>
<ul>
<li>PaddlePaddle的optimizer中支持L1Decay&#x2F;L2Decay。</li>
<li>PyTorch的optimizer支持不同参数列表的学习率分别设置，params传入字典即可，而PaddlePaddle的2.1.0版本目前尚未支持这种行为，可以通过设置<code>ParamAttr</code>的<code>learning_rate</code>参数，来确定相对学习率倍数。PaddlePaddle的2.2.0版本中虽然实现该功能，但是模型收敛速度较慢，不建议使用。<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Paddle/issues/36915">优化器收敛速度慢</a></li>
</ul>
<p><strong>【实战】</strong></p>
<p>本部分对齐建议对照<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/regularizer/L2Decay_cn.html">PaddlePaddle正则化API文档</a>与参考代码的优化器实现进行对齐，用之后的反向对齐统一验证该模块的正确性。</p>
<h3 id="3-8-反向对齐"><a href="#3-8-反向对齐" class="headerlink" title="3.8 反向对齐"></a>3.8 反向对齐</h3><p><strong>【基本流程】</strong></p>
<p>此处可以通过numpy生成假的数据和label（推荐），也可以准备固定的真实数据。具体流程如下。</p>
<ol>
<li>检查两个代码的训练超参数全部一致，如优化器及其超参数、学习率、BatchNorm&#x2F;LayerNorm中的eps等。</li>
<li>将PaddlePaddle与PyTorch网络中涉及的所有随机操作全部关闭，如dropout、drop_path等，推荐将模型设置为eval模式（<code>model.eval()</code>）</li>
<li>加载相同的weight dict（可以通过PyTorch来存储随机的权重），将准备好的数据分别传入网络并迭代，观察二者loss是否一致（此处batch-size要一致，如果使用多个真实数据，要保证传入网络的顺序一致）</li>
<li>如果经过2轮以上，loss均可以对齐，则基本可以认为反向对齐。</li>
</ol>
<p><strong>【注意事项】</strong></p>
<ul>
<li>如果第一轮loss就没有对齐，则需要仔细排查一下模型前向部分。</li>
<li>如果第二轮开始，loss开始无法对齐，则首先需要排查下超参数的差异，没问题的话，在<code>loss.backward()</code>方法之后，使用<code>tensor.grad</code>获取梯度值，二分的方法查找diff，定位出PaddlePaddle与PyTorch梯度无法对齐的API或者操作，然后进一步验证并反馈。</li>
</ul>
<p>梯度的打印方法示例代码如下所示，注释掉的内容即为打印网络中所有参数的梯度shape。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码地址：https://github.com/JunnYu/BERT-SST2-Prod/blob/2c372656bb1b077b0073c50161771d9fa9d8de5a/pipeline/Step4/test_bp.py#L12</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pd_train_some_iters</span>(<span class="hljs-params">model,</span><br><span class="hljs-params">                    criterion,</span><br><span class="hljs-params">                    optimizer,</span><br><span class="hljs-params">                    fake_data,</span><br><span class="hljs-params">                    fake_label,</span><br><span class="hljs-params">                    max_iter=<span class="hljs-number">2</span></span>):<br>    model = PDBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, num_classes=<span class="hljs-number">2</span>)<br>    classifier_weights = paddle.load(<span class="hljs-string">&quot;../classifier_weights/paddle_classifier_weights.bin&quot;</span>)<br>    model.load_dict(classifier_weights)<br>    model.<span class="hljs-built_in">eval</span>()<br>    criterion = paddle.nn.CrossEntropy()<br>    decay_params = [<br>        p.name <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters()<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-string">&quot;norm&quot;</span>])<br>    ]<br>    optimizer = paddle.optimizer.AdamW(learning_rate=<span class="hljs-number">3e-5</span>, parameters=model.parameters(),<br>        weight_decay=<span class="hljs-number">1e-2</span>,<br>        epsilon=<span class="hljs-number">1e-6</span>,<br>        apply_decay_param_fun=<span class="hljs-keyword">lambda</span> x: x <span class="hljs-keyword">in</span> decay_params)<br>    loss_list = []<br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iter):<br>        input_ids = paddle.to_tensor(fake_data)<br>        labels = paddle.to_tensor(fake_label)<br><br>        output = model(input_ids)<br>        loss = criterion(output, labels)<br>        loss.backward()<br>        optimizer.step()<br>        optimizer.clear_grad()<br>        loss_list.append(loss)<br>    <span class="hljs-keyword">return</span> loss_list<br></code></pre></td></tr></table></figure>



<p><strong>【实战】</strong></p>
<p>本部分可以参考文档：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step4/README.md#%E5%8F%8D%E5%90%91%E5%AF%B9%E9%BD%90%E6%93%8D%E4%BD%9C%E6%96%B9%E6%B3%95">反向对齐操作文档</a>。</p>
<p><strong>【验收】</strong></p>
<p>对于待复现的项目，反向对齐验收流程如下。</p>
<ol>
<li>输入：fake data &amp; label</li>
<li>输出：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle&#x2F;PyTorch：dict，key为tensor的name（自定义），value为具体loss的值。最后将dict使用reprod_log保存到各自的文件中，建议命名为<code>bp_align_paddle.npy</code>和<code>bp_align_torch.npy</code>。</li>
</ul>
</li>
</ul>
<ol>
<li>自测：使用reprod_log加载2个文件，使用report功能，记录结果到日志文件中，建议命名为<code>bp_align_diff_log.txt</code>，观察diff，二者diff小于特定的阈值即可。</li>
<li>提交内容：将<code>bp_align_paddle.npy</code>、<code>bp_align_torch.npy</code>与<code>bp_align_diff_log.txt</code>文件备份到<code>3.1节验收环节</code>新建的文件夹中，后续的输出结果和自查日志也放在该文件夹中，一并打包上传即可。</li>
<li>注意：</li>
</ol>
<ul>
<li><ul>
<li>loss需要保存至少2轮以上。</li>
<li>在迭代的过程中，需要保证模型的batch size等超参数完全相同</li>
<li>在迭代的过程中，需要设置<code>model.eval()</code>，使用固定的假数据，同时加载相同权重的预训练模型。</li>
</ul>
</li>
</ul>
<h3 id="3-9-训练集数据读取对齐"><a href="#3-9-训练集数据读取对齐" class="headerlink" title="3.9 训练集数据读取对齐"></a>3.9 训练集数据读取对齐</h3><p><strong>【基本流程】</strong></p>
<p>该部分内容与3.2节内容基本一致，参考PyTorch的代码，实现训练集数据读取与预处理模块即可。</p>
<p><strong>【注意事项】</strong></p>
<p>该部分内容，可以参考3.8节的自测方法，将输入的<code>fake data &amp; label</code>替换为训练的dataloader，但是需要注意的是：</p>
<ul>
<li>在使用train dataloader的时候，建议设置random seed，对于PyTorch来说</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#initialize random seed</span><br>torch.manual_seed(config.SEED)<br>torch.cuda.manual_seed_all(config.SEED)<br>np.random.seed(config.SEED)<br>random.seed(config.SEED)<br></code></pre></td></tr></table></figure>



<p>对于PaddlePaddle来说</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(config.SEED)<br>np.random.seed(config.SEED)<br>random.seed(config.SEED)<br></code></pre></td></tr></table></figure>

<h3 id="3-10-网络初始化对齐"><a href="#3-10-网络初始化对齐" class="headerlink" title="3.10 网络初始化对齐"></a>3.10 网络初始化对齐</h3><p><strong>【基本流程】</strong></p>
<ul>
<li>下面给出了部分初始化API的映射表。</li>
</ul>
<table>
<thead>
<tr>
<th>PaddlePaddle API</th>
<th>PyTorch API</th>
</tr>
</thead>
<tbody><tr>
<td>paddle.nn.initializer.KaimingNormal</td>
<td>torch.nn.init.kaiming_normal_</td>
</tr>
<tr>
<td>paddle.nn.initializer.KaimingUniform</td>
<td>torch.nn.init.kaiming_uniform_</td>
</tr>
<tr>
<td>paddle.nn.initializer.XavierNormal</td>
<td>torch.nn.init.xavier_normal_</td>
</tr>
<tr>
<td>paddle.nn.initializer.XavierUniform</td>
<td>torch.nn.init.xavier_uniform_</td>
</tr>
</tbody></table>
<p><strong>【注意事项】</strong></p>
<ul>
<li>更多初始化API可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.init.html">PyTorch初始化API文档</a>以及<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/Overview_cn.html#chushihuaxiangguan">PaddlePaddle初始化API文档</a>。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>本部分对齐建议对照<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/Overview_cn.html#chushihuaxiangguan">PaddlePaddle 初始化API文档</a>与参考代码的初始化实现对齐。</p>
<h3 id="3-11-模型训练对齐"><a href="#3-11-模型训练对齐" class="headerlink" title="3.11 模型训练对齐"></a>3.11 模型训练对齐</h3><p><strong>【基本流程】</strong></p>
<p>完成前面的步骤之后，就可以开始全量数据的训练对齐任务了。按照下面的步骤进行训练对齐。</p>
<ol>
<li>准备train&#x2F;eval data, loader, model</li>
<li>对model按照论文所述进行初始化(如果论文中提到加载了预训练模型，则按需加载pretrained model)</li>
<li>加载配置，开始训练，迭代得到最终模型与评估指标，将评估指标使用reprod_log保存到文件中。</li>
<li>将PaddlePaddle提供的参考指标使用reprod_log提交到另一个文件中。</li>
<li>使用reprod_log排查diff，小于阈值，即可完成自测。</li>
</ol>
<p><strong>【注意事项】</strong></p>
<ul>
<li><p>【强烈】建议先做完反向对齐之后再进行模型训练对齐，二者之间的不确定量包括：数据集、PaddlePaddle与参考代码在模型training mode下的区别，初始化参数。</p>
</li>
<li><p>在训练对齐过程中，受到较多随机量的影响，精度有少量diff是正常的，以SST-2数据集的分类为例，diff在0.15%以内可以认为是正常的，这里可以根据不同的任务，适当调整对齐检查的阈值(<code>ReprodDiffHelper.report</code>函数中的<code>diff_threshold</code>参数)。</p>
</li>
<li><p>训练过程中的波动是正常的，如果最终收敛结果不一致，可以 </p>
</li>
<li><ul>
<li>仔细排查Dropout、BatchNorm以及其他组网模块及超参是否无误。</li>
<li>基于参考代码随机生成一份预训练模型，转化为PaddlePaddle的模型，并使用PaddlePaddle加载训练，对比二者的收敛曲线与最终结果，排查初始化影响。</li>
<li>使用参考代码的Dataloader生成的数据，进行模型训练，排查train dataloader的影响。</li>
</ul>
</li>
</ul>
<p><strong>【实战】</strong></p>
<p>本部分可以参考文档：<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/Step5/README.md">训练对齐操作文档</a>。</p>
<p><strong>【验收】</strong></p>
<p>对于待复现的项目，训练对齐验收流程如下。</p>
<ol>
<li>输入：train&#x2F;eval dataloader, model</li>
<li>输出：</li>
</ol>
<ul>
<li><ul>
<li>PaddlePaddle：dict，key为保存值的name（自定义），value为具体评估指标的值。最后将dict使用reprod_log保存到文件中，建议命名为<code>train_align_paddle.npy</code>。</li>
<li>benchmark：dict，key为保存值的name（自定义），value为论文复现赛的评估指标要求的值。最后将dict使用reprod_log保存到文件中，建议命名为<code>train_align_benchmark.npy</code>。</li>
</ul>
</li>
</ul>
<ol>
<li>自测：使用reprod_log加载2个文件，使用report功能，记录结果到日志文件中，建议命名为<code>train_align_diff_log.txt</code>，观察diff，二者diff小于特定的阈值即可。</li>
<li>提交内容：将<code>train_align_paddle.npy</code>、<code>train_align_benchmark.npy</code>与<code>train_align_diff_log.txt</code>文件备份到<code>3.1节验收环节</code>新建的文件夹中，最终一并打包上传即可。</li>
</ol>
<h3 id="3-12-单机多卡训练"><a href="#3-12-单机多卡训练" class="headerlink" title="3.12 单机多卡训练"></a>3.12 单机多卡训练</h3><p>如果希望使用单机多卡提升训练效率，可以从以下几个过程对代码进行修改。</p>
<h4 id="3-12-1-数据读取"><a href="#3-12-1-数据读取" class="headerlink" title="3.12.1 数据读取"></a>3.12.1 数据读取</h4><p>对于PaddlePaddle来说，多卡数据读取这块主要的变化在sampler</p>
<p>对于单机单卡，sampler实现方式如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train_sampler = paddle.io.RandomSampler(dataset)<br>train_batch_sampler = paddle.io.BatchSampler(<br>    sampler=train_sampler, batch_size=args.batch_size)<br></code></pre></td></tr></table></figure>



<p>对于单机多卡任务，sampler实现方式如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">train_batch_sampler = paddle.io.DistributedBatchSampler(<br>        dataset=dataset,<br>        batch_size=args.batch_size,<br>        shuffle=<span class="hljs-literal">True</span>,<br>        drop_last=<span class="hljs-literal">False</span><br>    )<br></code></pre></td></tr></table></figure>



<p>注意：在这种情况下，单机多卡的代码仍然能够以单机单卡的方式运行，因此建议以这种sampler方式进行论文复现。</p>
<h4 id="3-12-2-多卡模型初始化"><a href="#3-12-2-多卡模型初始化" class="headerlink" title="3.12.2 多卡模型初始化"></a>3.12.2 多卡模型初始化</h4><p>如果以多卡的方式运行，需要初始化并行训练环境，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> paddle.distributed.get_world_size() &gt; <span class="hljs-number">1</span>:<br>        paddle.distributed.init_parallel_env()<br></code></pre></td></tr></table></figure>



<p>在模型组网并初始化参数之后，需要使用<code>paddle.DataParallel()</code>对模型进行封装，使得模型可以通过数据并行的模式被执行。代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> paddle.distributed.get_world_size() &gt; <span class="hljs-number">1</span>:<br>    model = paddle.DataParallel(model)<br></code></pre></td></tr></table></figure>

<h4 id="3-12-3-模型保存、日志保存等其他模块"><a href="#3-12-3-模型保存、日志保存等其他模块" class="headerlink" title="3.12.3 模型保存、日志保存等其他模块"></a>3.12.3 模型保存、日志保存等其他模块</h4><p>以模型保存为例，我们只需要在0号卡上保存即可，否则多个trainer同时保存的话，可能会造成写冲突，导致最终保存的模型不可用。</p>
<h4 id="3-12-4-程序启动方式"><a href="#3-12-4-程序启动方式" class="headerlink" title="3.12.4 程序启动方式"></a>3.12.4 程序启动方式</h4><p>对于单机单卡或者单机多卡的启动脚本可以参考：<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/bert">https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/bert</a></p>
<p>对于单机单卡，启动脚本如下所示</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">unset CUDA_VISIBLE_DEVICES<br>python -m paddle.distributed.launch --gpus &quot;0&quot; run_glue.py \<br>    --model_type bert \<br>    --model_name_or_path bert-base-uncased \<br>    --task_name SST-2 \<br>    --max_seq_length 128 \<br>    --batch_size 32   \<br>    --learning_rate 2e-5 \<br>    --num_train_epochs 3 \<br>    --logging_steps 1 \<br>    --save_steps 500 \<br>    --output_dir ./tmp/ \<br>    --device gpu \<br>    --use_amp False<br></code></pre></td></tr></table></figure>



<p>对于单机多卡（示例中为4卡训练），启动脚本如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">unset CUDA_VISIBLE_DEVICES<br>python -m paddle.distributed.launch --gpus &quot;0,1,2,3&quot; run_glue.py \<br>    --model_type bert \<br>    --model_name_or_path bert-base-uncased \<br>    --task_name SST-2 \<br>    --max_seq_length 128 \<br>    --batch_size 32   \<br>    --learning_rate 2e-5 \<br>    --num_train_epochs 3 \<br>    --logging_steps 1 \<br>    --save_steps 500 \<br>    --output_dir ./tmp/ \<br>    --device gpu \<br>    --use_amp False<br></code></pre></td></tr></table></figure>



<p>注意：这里4卡训练时，虽然单卡的batch size没有变化(32)，但是总卡的batch size相当于是单卡的4倍，因此学习率也设置为了单卡时的4倍。</p>
<p><strong>【实战】</strong></p>
<p>本部分可以参考paddlenlp库中的例子：<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/bert">单机多卡训练</a>。</p>
<h3 id="3-13-TIPC基础链条测试接入"><a href="#3-13-TIPC基础链条测试接入" class="headerlink" title="3.13 TIPC基础链条测试接入"></a>3.13 TIPC基础链条测试接入</h3><p><strong>【基本流程】</strong></p>
<ul>
<li><p>完成模型的训练、导出inference、基于PaddleInference的推理过程的文档与代码。参考链接： </p>
</li>
<li><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_paddle/README_cn.md">insightface训练预测使用文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/05_inference_deployment/inference/inference_cn.html">PaddleInference使用文档</a></li>
</ul>
</li>
<li><p>基于<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/models/blob/tipc/docs/tipc_test/development_specification_docs/train_infer_python.md">TIPC基础链条测试接入规范</a>，完成该模型的TIPC基础链条开发以及测试文档&#x2F;脚本，目录为<code>test_tipc</code>，测试脚本名称为<code>test_train_inference_python.sh</code>，该任务中只需要完成<code>少量数据训练模型，少量数据预测</code>的模式即可，用于测试TIPC流程的模型和少量数据需要放在当前repo中。</p>
</li>
</ul>
<p><strong>【注意事项】</strong></p>
<ul>
<li>基础链条测试接入时，只需要验证<code>少量数据训练模型，少量数据预测</code>的模式，只需要在Linux下验证通过即可。</li>
<li>在文档中需要给出一键测试的脚本与使用说明。</li>
</ul>
<p><strong>【实战】</strong></p>
<p>TIPC基础链条测试接入用例可以参考：<a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface/blob/master/recognition/arcface_paddle/test_tipc/readme.md">InsightFace-paddle TIPC基础链条测试开发文档</a>。</p>
<p><strong>【验收】</strong></p>
<ul>
<li>TIPC基础链条测试文档清晰，<code>test_train_inference_python.sh</code>脚本可以成功执行并返回正确结果。</li>
</ul>
<h2 id="4-论文复现注意事项与FAQ"><a href="#4-论文复现注意事项与FAQ" class="headerlink" title="4. 论文复现注意事项与FAQ"></a>4. 论文复现注意事项与FAQ</h2><h3 id="4-1-通用注意事项"><a href="#4-1-通用注意事项" class="headerlink" title="4.1 通用注意事项"></a>4.1 通用注意事项</h3><ul>
<li>需要仔细对照PaddlePaddle与参考代码的优化器参数实现，确保优化器参数严格对齐。</li>
<li>如果遇到一些Paddle不支持的API操作，可以尝试使用替代实现进行复现。如下面的PyTorch代码，PaddlePaddle中可以通过slice + concat API的组合形式进行功能实现。同时，对于这个问题，建议优先给Paddle提<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Paddle/issues/new/choose">ISSUE</a>，列出Paddle不支持的实现，开发人员会根据优先级进行开发。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.stack([<br>    per_locations[:, <span class="hljs-number">0</span>] - per_box_regression[:, <span class="hljs-number">0</span>],<br>    per_locations[:, <span class="hljs-number">1</span>] - per_box_regression[:, <span class="hljs-number">1</span>],<br>    per_locations[:, <span class="hljs-number">0</span>] + per_box_regression[:, <span class="hljs-number">2</span>],<br>    per_locations[:, <span class="hljs-number">1</span>] + per_box_regression[:, <span class="hljs-number">3</span>],<br>], dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>



<ul>
<li><p>如果遇到Paddle不包含的OP或者API，比如(1) 如果是某些算法实现存在调用了外部OP，而且Paddle也不包含该OP实现；(2) 其他框架存在的API或者OP，但是Paddle中没有这些OP。此时： </p>
</li>
<li><ul>
<li>对于Paddle资深用户来说，可以尝试使用Paddle的自定义算子功能，存在一定的代码开发量。</li>
<li>对于初学者来说，可以给Paddle提<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Paddle/issues/new/choose">ISSUE</a>，列出Paddle不支持的实现，Paddle开发人员会根据优先级进行实现。</li>
</ul>
</li>
<li><p>PaddlePaddle与PyTorch对于不同名称的API，实现的功能可能是相同的，复现的时候注意，比如<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/lr/StepDecay_cn.html#stepdecay">paddle.optimizer.lr.StepDecay</a>与<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR">torch.optim.lr_scheduler.StepLR</a> ，关于PaddlePaddle与PyTorch更多API的映射关系可以参考：<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_api_mapping/pytorch_api_mapping_cn.html">API映射表</a>。</p>
</li>
<li><p>对于PaddlePaddle来说，通过<code>paddle.set_device</code>函数（全局）来确定模型结构是运行在什么设备上，对于torch来说，是通过<code>model.to(&quot;device&quot;)</code> （局部）来确定模型结构的运行设备，这块在复现的时候需要注意。</p>
</li>
</ul>
<h3 id="4-2-模型结构对齐"><a href="#4-2-模型结构对齐" class="headerlink" title="4.2 模型结构对齐"></a>4.2 模型结构对齐</h3><h4 id="4-2-1-API"><a href="#4-2-1-API" class="headerlink" title="4.2.1 API"></a>4.2.1 API</h4><ul>
<li>对于 <code>paddle.nn.Linear</code> 层的weight参数，PaddlePaddle与PyTorch的保存方式不同，在转换时需要进行转置，示例代码可以参考<a target="_blank" rel="noopener" href="https://github.com/JunnYu/BERT-SST2-Prod/blob/main/pipeline/weights/torch2paddle.py">BERT权重转换脚本</a>。</li>
<li><code>torch.masked_fill</code>函数的功能目前可以使用<code>paddle.where</code>进行实现，可以参考：<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/faq/train_cn.html#paddletorch-masked-fillapi">链接</a>。</li>
<li><code>pack_padded_sequence</code>和<code>pad_packed_sequence</code>这两个API目前PaddlePaddle中没有实现，可以直接在RNN或者LSTM的输入中传入<code>sequence_length</code>来实现等价的功能。</li>
</ul>
<h4 id="4-2-2-权重转换"><a href="#4-2-2-权重转换" class="headerlink" title="4.2.2 权重转换"></a>4.2.2 权重转换</h4><ul>
<li>在权重转换的时候，不能只关注参数的名称，比如说有些<code>paddle.nn.Linear</code>层，但是定义的变量名称为<code>conv</code>，这种也是需要进行权重转置的。</li>
<li>权重转换时，建议同时打印 Paddle 和 PyTorch 对应权重的shape，以防止名称相似但是shape不同的参数权重转换报错。</li>
</ul>
<h4 id="4-2-3-模型组网正确性验证"><a href="#4-2-3-模型组网正确性验证" class="headerlink" title="4.2.3 模型组网正确性验证"></a>4.2.3 模型组网正确性验证</h4><ul>
<li>在论文复现的过程中，可能会遇到一些经典的模型结构，比如Transformer等，Paddle官方也提供了Transformer的实现，但是这里建议自己根据PyTorch代码重新实现一遍，一方面是对整体的模型结构更加熟悉，另一方面也保证模型结构和权重完全对齐。</li>
<li>在复杂的网络结构中，如果前向结果对不齐，可以按照模块排查问题，比如依次获取embedding、transformer-block、mlm-head输出等，看下问题具体出现在哪个子模块，再进到子模块详细排查。</li>
<li>网络结构对齐后，尽量使用训练好的预训练模型和真实的数据进行前向diff计算，这样更准确。</li>
</ul>
<h3 id="4-3-验证-测试集数据读取对齐"><a href="#4-3-验证-测试集数据读取对齐" class="headerlink" title="4.3 验证&#x2F;测试集数据读取对齐"></a>4.3 验证&#x2F;测试集数据读取对齐</h3><ul>
<li>需要仔细排查数据预处理，不仅包含的预处理方法相同，也需要保证预处理的流程相同，比如padding策略不同和截断策略的不同会导致得到最终的结果是不同的。</li>
</ul>
<h3 id="4-4-评估指标对齐"><a href="#4-4-评估指标对齐" class="headerlink" title="4.4 评估指标对齐"></a>4.4 评估指标对齐</h3><ul>
<li>真实数据评估时，需要注意评估时 <code>paddle.io.DataLoader</code> 的 <code>drop_last</code> 参数是否打开(文档<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html#dataloader">链接</a>)，复现代码需要与参考代码保持一致，否则最后不够batch-size的数据的评估会有diff。</li>
<li>在识别或者检索过程中，为了加速评估过程，往往会将评估函数由CPU实现改为GPU实现，由此会带来评估函数输出的不一致。这是由于sort函数对于相同值的排序结果不同带来的。在复现的过程中，如果可以接受轻微的指标不稳定，可以使用PaddlePaddle的sort函数，如果对于指标非常敏感，同时对速度性能要求很高，可以给PaddlePaddle提<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Paddle/issues/new/choose">ISSUE</a>，由研发人员高优开发。</li>
</ul>
<h3 id="4-5-损失函数对齐"><a href="#4-5-损失函数对齐" class="headerlink" title="4.5 损失函数对齐"></a>4.5 损失函数对齐</h3><ul>
<li>部分算法的损失函数中会用到 bool 索引，这时候可以使用<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/where_cn.html#where">paddle.where</a> 代替。</li>
<li><code>paddle.nn.CrossEntropyLoss</code> 默认是在最后一维(axis&#x3D;-1)计算损失函数，而 <code>torch.nn.CrossEntropyLoss</code> 是在axis&#x3D;1的地方计算损失函数，因此如果输入的维度大于2，这里需要保证计算的维(axis)相同，否则可能会出错。</li>
<li>在生成模型中会遇到梯度损失，需要对模型中的算子求二次梯度，目前<code>MaxPooling</code>暂时不支持二次梯度，如果复现的过程中遇到了需要对<code>MaxPooling</code>求二次梯度的情况，可以和Paddle官方开发同学反馈，进一步确认解决方案。</li>
<li>在保存损失函数值的时候，注意要使用<code>paddle.no_grad</code>，或者仅仅保存转换成 numpy 的数组，避免损失没有析构导致内存泄漏问题。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 错误示范</span><br>loss = celoss(pred, label)<br>avg_loss += loss<br><span class="hljs-comment"># 正确示范1</span><br>loss = celoss(pred, label)<br>avg_loss += loss.numpy()<br><span class="hljs-comment"># 正确示范2</span><br>loss = celoss(pred, label)<br><span class="hljs-keyword">with</span> paddle.no_grad()<br>    avg_loss += loss<br></code></pre></td></tr></table></figure>

<h3 id="4-6-优化器对齐"><a href="#4-6-优化器对齐" class="headerlink" title="4.6 优化器对齐"></a>4.6 优化器对齐</h3><ul>
<li>Paddle目前支持在 <code>optimizer</code> 中通过设置 <code>params_groups</code> 的方式设置不同参数的更新方式，可以参考<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/optimizer/optimizer.py#L107">代码示例</a> 。</li>
<li>有些模型训练时，会使用梯度累加策略，即累加到一定step数量之后才进行参数更新，这时在实现上需要注意对齐。</li>
<li>在某些任务中，比如说深度学习可视化、可解释性等任务中，一般只要求模型前向过程，不需要训练，此时优化器、学习率等用于模型训练的模块对于该类论文复现是不需要的。</li>
<li>在文本分类领域，大多数Transformer模型都采用了AdamW优化器，并且会设置weigh decay，同时部分参数设置为no weight decay，例如位置编码的参数通常设置为no weight decay，no weight decay参数设置不正确，最终会有明显的精度损失，需要特别注意。一般可以通过分析模型权重来发现该问题，分别计算官方模型和复现模型每层参数权重的平均值、方差，对每一层依次对比，有显著差异的层可能存在问题，因为在weight decay的作用下，参数权重数值会相对较小，而未正确设置no weight decay，则会造成该层参数权重数值异常偏小。</li>
</ul>
<h3 id="4-7-学习率对齐"><a href="#4-7-学习率对齐" class="headerlink" title="4.7 学习率对齐"></a>4.7 学习率对齐</h3><ul>
<li>PaddlePaddle 中参数的学习率受到优化器学习率和<code>ParamAttr</code>中设置的学习率影响，因此跟踪学习率需要将二者结合进行跟踪。</li>
<li>对于复现代码和参考代码，学习率在整个训练过程中在相同的轮数相同的iter下应该保持一致，可以通过<code>reprod_log</code>工具、打印学习率值或者可视化二者学习率的log来查看diff。</li>
<li>有些网络的学习率策略比较细致，比如带warmup的学习率策略，这里需要保证起始学习率等参数都完全一致。</li>
</ul>
<h3 id="4-8-正则化策略对齐"><a href="#4-8-正则化策略对齐" class="headerlink" title="4.8 正则化策略对齐"></a>4.8 正则化策略对齐</h3><ul>
<li>在如Transformer或者少部分CNN模型中，存在一些参数不做正则化(正则化系数为0)的情况。这里需要找到这些参数并对齐取消实施正则化策略，可以参考<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.3/ppcls/arch/backbone/model_zoo/resnest.py#L72">这里</a>，对特定参数进行修改。</li>
</ul>
<h3 id="4-9-反向对齐"><a href="#4-9-反向对齐" class="headerlink" title="4.9 反向对齐"></a>4.9 反向对齐</h3><ul>
<li>反向对齐时，如果第二轮开始，loss开始无法对齐，则首先需要排查下超参数的差异，没问题的话，在<code>loss.backward()</code>方法之后，使用<code>tensor.grad</code>获取梯度值，二分的方法查找diff，定位出PaddlePaddle与PyTorch梯度无法对齐的API或者操作，然后进一步验证。第3章中给出了获取所有参数的梯度方法，如果只希望打印特定参数的梯度，可以用下面的方式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_hook_fn</span>(<span class="hljs-params">grad</span>):<br>    <span class="hljs-built_in">print</span>(grad)<br><br>x = paddle.to_tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], stop_gradient=<span class="hljs-literal">False</span>)<br>h = x.register_hook(print_hook_fn)<br>w = x * <span class="hljs-number">4</span><br>w.backward()<br><span class="hljs-comment"># backward之后会输出下面的内容</span><br><span class="hljs-comment">#     Tensor(shape=[4], dtype=float32, place=CPUPlace, stop_gradient=False,</span><br><span class="hljs-comment">#            [4., 4., 4., 4.])</span><br></code></pre></td></tr></table></figure>

<h3 id="4-10-训练集数据读取对齐"><a href="#4-10-训练集数据读取对齐" class="headerlink" title="4.10 训练集数据读取对齐"></a>4.10 训练集数据读取对齐</h3><h4 id="4-10-1-API"><a href="#4-10-1-API" class="headerlink" title="4.10.1 API"></a>4.10.1 API</h4><ul>
<li>在前向过程中，如果数据预处理过程运行出错，请先将 <code>paddle.io.DataLoader</code> 的 <code>num_workers</code> 参数设为0，然后根据单个进程下的报错日志定位出具体的bug。</li>
</ul>
<h4 id="4-10-2-数据预处理"><a href="#4-10-2-数据预处理" class="headerlink" title="4.10.2 数据预处理"></a>4.10.2 数据预处理</h4><ul>
<li>如果数据处理过程中涉及到随机数生成，建议固定seed (<code>np.random.seed(0)</code>, <code>random.seed(0)</code>)，查看复现代码和参考代码处理后的数据是否有diff。</li>
<li>对文本进行tokenizer处理时，需要确定文本的截断策略，padding策略。</li>
</ul>
<h3 id="4-11-网络初始化对齐"><a href="#4-11-网络初始化对齐" class="headerlink" title="4.11 网络初始化对齐"></a>4.11 网络初始化对齐</h3><ul>
<li>对于不同的深度学习框架，网络初始化在大多情况下，即使值的分布完全一致，也无法保证值完全一致，这里也是论文复现中不确定性比较大的地方。如果十分怀疑初始化导致的问题，建议将参考的初始化权重转成paddle模型，加载该初始化模型训练，看下收敛精度。</li>
<li>CNN对于模型初始化相对来说没有那么敏感，在迭代轮数与数据集足够的情况下，最终精度指标基本接近；而transformer系列模型对于初始化比较敏感，在transformer系列模型训练对齐过程中，建议对这一块进行重点检查。</li>
</ul>
<h3 id="4-12-模型训练对齐"><a href="#4-12-模型训练对齐" class="headerlink" title="4.12 模型训练对齐"></a>4.12 模型训练对齐</h3><h4 id="4-12-1-训练对齐通用问题"><a href="#4-12-1-训练对齐通用问题" class="headerlink" title="4.12.1 训练对齐通用问题"></a>4.12.1 训练对齐通用问题</h4><ul>
<li><p>有条件的话，复现工作之前最好先基于官方代码完成训练，保证与官方指标能够对齐，并且将训练策略和训练过程中的关键指标记录保存下来，比如每个epoch的学习率、Train Loss、Eval Loss、Eval Acc等，在复现网络的训练过程中，将关键指标保存下来，这样可以将两次训练中关键指标的变化曲线绘制出来，能够很方便的进行对比。</p>
</li>
<li><p>训练过程中可以对loss或者acc进行可视化，和竞品loss或者acc进行直观的对比；如果训练较大的数据集，1次完整训练的成本比较高，此时可以隔一段时间查看一下，如果精度差异比较大，建议先停掉实验，排查原因。</p>
</li>
<li><p>如果训练的过程中出nan，一般是因为除0或者log0的情况， 可以着重看下几个部分： </p>
</li>
<li><ul>
<li>如果有预训练模型的话，可以确认下是否加载正确</li>
<li>模型结构中计算loss的部分是否有考虑到正样本为0的情况</li>
<li>也可能是某个API的数值越界导致的，可以测试较小的输入是否还会出现nan。</li>
</ul>
</li>
<li><p>如果训练过程中如果出现不收敛的情况，可以 </p>
</li>
<li><ul>
<li>简化网络和数据，实验是否收敛；</li>
<li>如果是基于原有实现进行改动，可以尝试控制变量法，每次做一个改动，逐个排查；</li>
<li>检查学习率是否过大、优化器设置是否合理，排查下weight decay是否设置正确；</li>
<li>保存不同step之间的模型参数，观察模型参数是否更新。</li>
</ul>
</li>
</ul>
<h4 id="4-12-2-细分场景特定问题"><a href="#4-12-2-细分场景特定问题" class="headerlink" title="4.12.2 细分场景特定问题"></a>4.12.2 细分场景特定问题</h4><ul>
<li>小数据上指标波动可能比较大，时间允许的话，可以跑多次实验，取平均值。</li>
</ul>
<h3 id="4-13-TIPC基础链条测试接入"><a href="#4-13-TIPC基础链条测试接入" class="headerlink" title="4.13 TIPC基础链条测试接入"></a>4.13 TIPC基础链条测试接入</h3><ul>
<li>在接入时，建议将少量用于测试的数据打包(<code>tar -zcf lite_data.tar data/</code>)，放在data目录下，后续在进行环境准备的时候，直接解压该压缩包即可。</li>
<li>接入过程中，需要依赖于inference模型，因此建议首先提供模型导出和基于inference模型的预测脚本，之后再接入TIPC测试代码与文档。</li>
</ul>
<h2 id="来源："><a href="#来源：" class="headerlink" title="来源："></a>来源：</h2><p><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/">https://github.com/PaddlePaddle/</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">xie lei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/01/10/NLP%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-paddle%E6%8C%87%E5%8D%97/">http://example.com/2024/01/10/NLP%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-paddle%E6%8C%87%E5%8D%97/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Xielei's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/paddle/">paddle</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="/img/logo.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/15/%E5%87%BD%E6%95%B0%E5%BC%8F%E3%80%81%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/" title="函数式编程与响应式编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">函数式编程与响应式编程</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/10/CV%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8C%87%E5%8D%97/" title="CV论文复现-paddle指南"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CV论文复现-paddle指南</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/10/CV%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8C%87%E5%8D%97/" title="CV论文复现-paddle指南"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-10</div><div class="title">CV论文复现-paddle指南</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">xie lei</div><div class="author-info__description">朝闻道兮</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xieleixielei"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E8%B5%9B%E6%8C%87%E5%8D%97-NLP%E6%96%B9%E5%90%91"><span class="toc-number">1.</span> <span class="toc-text">论文复现赛指南-NLP方向</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%80%BB%E8%A7%88"><span class="toc-number">1.1.</span> <span class="toc-text">1. 总览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E8%83%8C%E6%99%AF"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%89%8D%E5%BA%8F%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 前序工作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B4%E4%BD%93%E6%A1%86%E5%9B%BE"><span class="toc-number">1.2.</span> <span class="toc-text">2. 整体框图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 流程概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-reprod-log-whl%E5%8C%85"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 reprod_log whl包</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-reprod-log%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1 reprod_log工具简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-reprod-log%E4%BD%BF%E7%94%A8demo"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2 reprod_log使用demo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-reprod-log%E5%9C%A8%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E4%B8%AD%E5%BA%94%E7%94%A8"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3 reprod_log在论文复现中应用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%8F%8A%E5%AE%9E%E6%88%98"><span class="toc-number">1.3.</span> <span class="toc-text">3. 论文复现理论知识及实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 模型结构对齐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E4%BB%A3%E7%A0%81%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">3.1.1 网络结构代码转换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E6%9D%83%E9%87%8D%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">3.1.2 权重转换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E6%A8%A1%E5%9E%8B%E7%BB%84%E7%BD%91%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">3.1.3 模型组网正确性验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%AA%8C%E8%AF%81-%E6%B5%8B%E8%AF%95%E9%9B%86%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 验证&#x2F;测试集数据读取对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 评估指标对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 损失函数对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E4%BC%98%E5%8C%96%E5%99%A8%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 优化器对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.6.</span> <span class="toc-text">3.6 学习率对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E6%AD%A3%E5%88%99%E5%8C%96%E7%AD%96%E7%95%A5%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.7.</span> <span class="toc-text">3.7 正则化策略对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-%E5%8F%8D%E5%90%91%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.8.</span> <span class="toc-text">3.8 反向对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.9.</span> <span class="toc-text">3.9 训练集数据读取对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-10-%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.10.</span> <span class="toc-text">3.10 网络初始化对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-11-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%AF%B9%E9%BD%90"><span class="toc-number">1.3.11.</span> <span class="toc-text">3.11 模型训练对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-12-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.12.</span> <span class="toc-text">3.12 单机多卡训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-12-1-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="toc-number">1.3.12.1.</span> <span class="toc-text">3.12.1 数据读取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-12-2-%E5%A4%9A%E5%8D%A1%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.3.12.2.</span> <span class="toc-text">3.12.2 多卡模型初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-12-3-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E3%80%81%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98%E7%AD%89%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97"><span class="toc-number">1.3.12.3.</span> <span class="toc-text">3.12.3 模型保存、日志保存等其他模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-12-4-%E7%A8%8B%E5%BA%8F%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.12.4.</span> <span class="toc-text">3.12.4 程序启动方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-13-TIPC%E5%9F%BA%E7%A1%80%E9%93%BE%E6%9D%A1%E6%B5%8B%E8%AF%95%E6%8E%A5%E5%85%A5"><span class="toc-number">1.3.13.</span> <span class="toc-text">3.13 TIPC基础链条测试接入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%E4%B8%8EFAQ"><span class="toc-number">1.4.</span> <span class="toc-text">4. 论文复现注意事项与FAQ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%80%9A%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 通用注意事项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 模型结构对齐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-API"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-%E6%9D%83%E9%87%8D%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 权重转换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-%E6%A8%A1%E5%9E%8B%E7%BB%84%E7%BD%91%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 模型组网正确性验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%AA%8C%E8%AF%81-%E6%B5%8B%E8%AF%95%E9%9B%86%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 验证&#x2F;测试集数据读取对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 评估指标对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 损失函数对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E4%BC%98%E5%8C%96%E5%99%A8%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 优化器对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.7.</span> <span class="toc-text">4.7 学习率对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-%E6%AD%A3%E5%88%99%E5%8C%96%E7%AD%96%E7%95%A5%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.8.</span> <span class="toc-text">4.8 正则化策略对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-%E5%8F%8D%E5%90%91%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.9.</span> <span class="toc-text">4.9 反向对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-10-%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.10.</span> <span class="toc-text">4.10 训练集数据读取对齐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-10-1-API"><span class="toc-number">1.4.10.1.</span> <span class="toc-text">4.10.1 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-10-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.4.10.2.</span> <span class="toc-text">4.10.2 数据预处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-11-%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.11.</span> <span class="toc-text">4.11 网络初始化对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-12-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%AF%B9%E9%BD%90"><span class="toc-number">1.4.12.</span> <span class="toc-text">4.12 模型训练对齐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-12-1-%E8%AE%AD%E7%BB%83%E5%AF%B9%E9%BD%90%E9%80%9A%E7%94%A8%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.12.1.</span> <span class="toc-text">4.12.1 训练对齐通用问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-12-2-%E7%BB%86%E5%88%86%E5%9C%BA%E6%99%AF%E7%89%B9%E5%AE%9A%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.12.2.</span> <span class="toc-text">4.12.2 细分场景特定问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-13-TIPC%E5%9F%BA%E7%A1%80%E9%93%BE%E6%9D%A1%E6%B5%8B%E8%AF%95%E6%8E%A5%E5%85%A5"><span class="toc-number">1.4.13.</span> <span class="toc-text">4.13 TIPC基础链条测试接入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A5%E6%BA%90%EF%BC%9A"><span class="toc-number">1.5.</span> <span class="toc-text">来源：</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/14/%E9%9D%92%E8%AE%AD%E8%90%A5-%E5%B0%8FR%E7%9A%84%E5%9F%8E%E5%B8%82%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%8C%91%E6%88%98/" title="青训营-小R的城市稳定性挑战">青训营-小R的城市稳定性挑战</a><time datetime="2024-11-14T02:19:58.000Z" title="发表于 2024-11-14 10:19:58">2024-11-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/13/%E9%9D%92%E8%AE%AD%E8%90%A5-%E5%B0%8FR%E7%9A%84%E4%B8%89%E7%9A%84%E5%80%8D%E6%95%B0%E5%88%86%E8%A7%A3/" title="青训营-小R的三的倍数分解">青训营-小R的三的倍数分解</a><time datetime="2024-11-13T07:57:40.000Z" title="发表于 2024-11-13 15:57:40">2024-11-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/70-%E7%88%AC%E6%A5%BC%E6%A2%AF/" title="70. 爬楼梯">70. 爬楼梯</a><time datetime="2024-07-16T12:07:31.000Z" title="发表于 2024-07-16 20:07:31">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/19-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E5%80%92%E6%95%B0%E7%AC%AC-N-%E4%B8%AA%E7%BB%93%E7%82%B9/" title="19. 删除链表的倒数第 N 个结点">19. 删除链表的倒数第 N 个结点</a><time datetime="2024-07-16T08:00:48.000Z" title="发表于 2024-07-16 16:00:48">2024-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/16/2956-%E6%89%BE%E5%88%B0%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E5%85%83%E7%B4%A0/" title="2956.找到两个数组中的公共元素">2956.找到两个数组中的公共元素</a><time datetime="2024-07-16T07:28:48.000Z" title="发表于 2024-07-16 15:28:48">2024-07-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By xie lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>